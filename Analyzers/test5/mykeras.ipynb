{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using numpy 1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using keras 2.0.5\n",
      "[INFO] Using sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "import random\n",
    "random.seed(2023)\n",
    "print('[INFO] Using numpy {0}'.format(np.__version__))\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import initializers, regularizers, optimizers, losses\n",
    "print('[INFO] Using keras {0}'.format(keras.__version__))\n",
    "\n",
    "import sklearn\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('[INFO] Using sklearn {0}'.format(sklearn.__version__))\n",
    "\n",
    "import pandas as pd\n",
    "#import statsmodels\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Globals\n",
    "nlayers = 25  # 13 (CSC) + 9 (RPC) + 3 (GEM)\n",
    "\n",
    "infile = '../test2/histos_tba.8.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded the variables with shape (3953523, 103)\n",
      "[INFO] Loaded the parameters with shape (3953523, 3)\n"
     ]
    }
   ],
   "source": [
    "#### Load data ####\n",
    "\n",
    "try:\n",
    "    loaded = np.load(infile)\n",
    "    the_parameters = loaded['parameters']\n",
    "    the_variables = loaded['variables']\n",
    "except:\n",
    "    print('[ERROR] Failed to load data from file: {0}'.format(infile))\n",
    "\n",
    "print('[INFO] Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "print('[INFO] Loaded the parameters with shape {0}'.format(the_parameters.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(object):\n",
    "\n",
    "  def __init__(self, x, y):\n",
    "    if x is not None and y is not None:\n",
    "      assert(x.shape[1] == (nlayers * 4) + 3)\n",
    "      assert(y.shape[1] == 3)\n",
    "      assert(x.shape[0] == y.shape[0])\n",
    "\n",
    "      self.nentries = x.shape[0]\n",
    "      self.x_orig  = x\n",
    "      self.y_orig  = y\n",
    "      self.x_copy  = x.copy()\n",
    "      self.y_copy  = y.copy()\n",
    "\n",
    "      # Get views\n",
    "      self.x_phi   = self.x_copy[:, nlayers*0:nlayers*1]\n",
    "      self.x_theta = self.x_copy[:, nlayers*1:nlayers*2]\n",
    "      self.x_bend  = self.x_copy[:, nlayers*2:nlayers*3]\n",
    "      self.x_mask  = self.x_copy[:, nlayers*3:nlayers*4].astype(np.bool)  # this makes a copy\n",
    "      self.x_road  = self.x_copy[:, nlayers*4:nlayers*5]  # ipt, ieta, iphi\n",
    "      self.y_pt    = self.y_copy[:, 0]  # q/pT\n",
    "      self.y_phi   = self.y_copy[:, 1]\n",
    "      self.y_eta   = self.y_copy[:, 2]\n",
    "      \n",
    "      # Make event weight\n",
    "      #self.w       = np.ones(self.y_pt.shape, dtype=np.float32)\n",
    "      self.w       = np.abs(self.y_pt)/0.2 + 1.0\n",
    "      \n",
    "      # Subtract median phi from hit phis\n",
    "      #self.x_phi_median    = self.x_road[:, 2] * 32 - 16  # multiply by 'quadstrip' unit (4 * 8)\n",
    "      self.x_phi_median    = self.x_road[:, 2] * 16 - 8  # multiply by 'doublestrip' unit (2 * 8)\n",
    "      self.x_phi_median    = self.x_phi_median[:, np.newaxis]\n",
    "      self.x_phi          -= self.x_phi_median\n",
    "      \n",
    "      # Subtract median theta from hit thetas\n",
    "      self.x_theta_median  = np.nanmedian(self.x_theta, axis=1)\n",
    "      self.x_theta_median  = self.x_theta_median[:, np.newaxis]\n",
    "      self.x_theta        -= self.x_theta_median\n",
    "      \n",
    "      # Zones\n",
    "      self.x_ieta  = self.x_road[:, 1].astype(np.int32)\n",
    "      \n",
    "      # Standard scales\n",
    "      self.x_mean  = np.nanmean(self.x_copy, axis=0)\n",
    "      self.x_std   = np.nanstd(self.x_copy, axis=0)\n",
    "      self.x_std   = self._handle_zero_in_scale(self.x_std)\n",
    "      self.x_copy -= self.x_mean\n",
    "      self.x_copy /= self.x_std\n",
    "      \n",
    "      ## Remove outlier hits by checking hit thetas\n",
    "      #x_theta_tmp = np.abs(self.x_theta) > 4.0\n",
    "      #self.x_phi  [x_theta_tmp] = np.nan\n",
    "      #self.x_theta[x_theta_tmp] = np.nan\n",
    "      #self.x_bend [x_theta_tmp] = np.nan\n",
    "      #self.x_mask [x_theta_tmp] = 1.0\n",
    "      \n",
    "      ## Something wrong with GE2/1?\n",
    "      #bad_ge21 = 23\n",
    "      #self.x_phi  [:, bad_ge21] = np.nan\n",
    "      #self.x_theta[:, bad_ge21] = np.nan\n",
    "      #self.x_bend [:, bad_ge21] = np.nan\n",
    "      #self.x_mask [:, bad_ge21] = 1.0\n",
    "      \n",
    "      # Add variables: theta_median and mode variables\n",
    "      self.x_theta_median -= 3  # scaled to [0,1]\n",
    "      self.x_theta_median /= 83\n",
    "      hits_to_station = np.array((5,5,1,1,1,2,2,2,2,3,3,4,4,1,1,2,3,3,3,4,4,4,5,2,1), dtype=np.int32)  # '5' denotes ME1/1\n",
    "      assert(len(hits_to_station) == nlayers)\n",
    "      self.x_mode_vars = np.zeros((self.nentries, 5), dtype=np.float32)\n",
    "      self.x_mode_vars[:,0] = np.any(self.x_mask[:,hits_to_station == 5] == 0, axis=1)\n",
    "      self.x_mode_vars[:,1] = np.any(self.x_mask[:,hits_to_station == 1] == 0, axis=1)\n",
    "      self.x_mode_vars[:,2] = np.any(self.x_mask[:,hits_to_station == 2] == 0, axis=1)\n",
    "      self.x_mode_vars[:,3] = np.any(self.x_mask[:,hits_to_station == 3] == 0, axis=1)\n",
    "      self.x_mode_vars[:,4] = np.any(self.x_mask[:,hits_to_station == 4] == 0, axis=1)\n",
    "      \n",
    "      # Remove NaN\n",
    "      #np.nan_to_num(self.x_copy, copy=False)\n",
    "      self.x_copy[np.isnan(self.x_copy)] = 0.0\n",
    "\n",
    "  # Copied from scikit-learn\n",
    "  def _handle_zero_in_scale(self, scale):\n",
    "    scale[scale == 0.0] = 1.0\n",
    "    return scale\n",
    "\n",
    "  def get_x(self):\n",
    "    #x_new = self.x_phi\n",
    "    x_new = np.hstack((self.x_phi, self.x_theta, self.x_bend, self.x_theta_median, self.x_mode_vars))\n",
    "    return x_new\n",
    "\n",
    "  def get_x_mask(self):\n",
    "    x_mask = self.x_mask.copy()\n",
    "    return x_mask\n",
    "\n",
    "  def get_y(self):\n",
    "    y_new = self.y_pt.copy()\n",
    "    return y_new\n",
    "\n",
    "  def get_w(self):\n",
    "    w_new = self.w.copy()\n",
    "    return w_new\n",
    "  \n",
    "  def get_x_zone(self, zone):\n",
    "    x_ieta_tmp = (self.x_ieta == zone)\n",
    "    x_new = np.hstack((self.x_phi[x_ieta_tmp], self.x_theta[x_ieta_tmp], self.x_bend[x_ieta_tmp], self.x_theta_median[x_ieta_tmp], self.x_mode_vars[x_ieta_tmp]))\n",
    "    return x_new\n",
    "  \n",
    "  def get_x_mask_zone(self, zone):\n",
    "    x_ieta_tmp = (self.x_ieta == zone)\n",
    "    x_mask = self.x_mask[x_ieta_tmp]\n",
    "    return x_mask\n",
    "\n",
    "  def save_encoder(self, filepath):\n",
    "    np.savez_compressed(filepath, x_mean=self.x_mean, x_std=self.x_std)\n",
    "\n",
    "  def load_endcoder(self, filepath):\n",
    "    loaded = np.load(filepath)\n",
    "    self.x_mean = loaded['x_mean']\n",
    "    self.x_std = loaded['x_std']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs/5.0-ghjeda6/lib/python2.7/site-packages/ipykernel_launcher.py:44: RuntimeWarning: Mean of empty slice\n",
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-numpy/1.12.1-mlhled2/lib/python2.7/site-packages/numpy-1.12.1-py2.7-linux-x86_64.egg/numpy/lib/nanfunctions.py:1423: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Encoder is saved as encoder.npz\n",
      "[INFO] Using 81 variables and 1 parameters\n"
     ]
    }
   ],
   "source": [
    "#### Prepare data ####\n",
    "\n",
    "# Preprocess data\n",
    "encoder = Encoder(the_variables, the_parameters)\n",
    "x, y, w, x_mask = encoder.get_x(), encoder.get_y(), encoder.get_w(), encoder.get_x_mask()\n",
    "encoder.save_encoder('encoder.npz')\n",
    "print('[INFO] Encoder is saved as encoder.npz')\n",
    "\n",
    "# Split dataset in training and testing\n",
    "x_train, x_test, y_train, y_test, w_train, w_test, x_mask_train, x_mask_test = train_test_split(x, y, w, x_mask, test_size=0.4)\n",
    "\n",
    "nvariables = x_train.shape[1]\n",
    "nparameters = 1\n",
    "print('[INFO] Using {0} variables and {1} parameters'.format(nvariables, nparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69595063  0.          0.         ...,  1.          1.          0.        ]\n",
      " [-0.33015883  0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.33125722 ...,  1.          1.          1.        ]\n",
      " ..., \n",
      " [ 0.92240101  0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.2614204   0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.          0.66696632 ...,  1.          1.          1.        ]] [-0.10976338 -0.03732378  0.04606546 ...,  0.1067722   0.11707978\n",
      "  0.07033174] [ 1.54881692  1.18661892  1.23032737 ...,  1.53386092  1.58539891\n",
      "  1.3516587 ]\n",
      "[ -1.16163479e-08  -1.63351732e-08   6.16444273e-09  -8.79568596e-09\n",
      "   0.00000000e+00  -8.18791008e-08   1.42959280e-07   1.23685329e-08\n",
      "   1.67215717e-08   2.30435646e-07  -9.36387821e-08   6.49591385e-08\n",
      "   2.16115367e-07   9.15457754e-09  -8.05541944e-08   9.15777250e-08\n",
      "  -8.72471517e-08   1.18372220e-07  -1.65068244e-08  -4.74757522e-09\n",
      "  -3.86881055e-07  -1.50894991e-07   6.82282177e-08  -7.64228290e-08\n",
      "   0.00000000e+00  -3.13735040e-06  -1.39440544e-07   9.14745470e-08\n",
      "   3.55130709e-07   0.00000000e+00  -6.42371674e-07   2.17294792e-06\n",
      "   3.57744852e-07  -7.32120782e-08   5.72325007e-07   1.33653191e-06\n",
      "  -1.03734988e-06   5.84144573e-07   2.83359758e-09  -2.20865326e-09\n",
      "  -7.41399546e-08  -1.01179197e-07  -1.77814812e-07  -7.01304685e-08\n",
      "  -1.67456164e-07   8.29823591e-07  -4.29457288e-08  -3.96127263e-07\n",
      "  -2.70409288e-07   0.00000000e+00  -6.09789538e-07  -1.15684065e-06\n",
      "   9.89658986e-08   5.51721087e-07   0.00000000e+00   8.82417694e-08\n",
      "   3.27862352e-07   7.63964294e-07  -7.86095654e-07   3.87029360e-07\n",
      "  -1.97373561e-06   2.09788982e-06   1.68049371e-06   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   4.17245507e-01\n",
      "   7.09960222e-01   3.00389290e-01   9.51365650e-01   9.82467294e-01\n",
      "   9.83936608e-01]\n",
      "[ 0.60055399  0.59068108  0.37518054  0.3903282   0.          0.58156157\n",
      "  0.57416964  0.35733905  0.37317061  0.73193932  0.58793187  0.67424315\n",
      "  0.64615089  0.33778793  0.34803733  0.50378847  0.66906643  0.51004124\n",
      "  0.33962953  0.63347912  0.52874434  0.41107425  0.67909759  0.67624182\n",
      "  0.          0.59710217  0.59001392  0.37484854  0.39061305  0.\n",
      "  0.57922316  0.57383871  0.35741144  0.37291867  0.7244671   0.58749181\n",
      "  0.67582595  0.64324337  0.33783507  0.34821653  0.50425774  0.67258942\n",
      "  0.50972843  0.33976164  0.63349867  0.52971441  0.41116533  0.67801434\n",
      "  0.6743477   0.          0.60019904  0.59029901  0.37511086  0.39034617\n",
      "  0.          0.58228308  0.57273251  0.35697249  0.37267894  0.7318542\n",
      "  0.58795804  0.67457533  0.64585036  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.26023671  0.44838247  0.45397145  0.21228048  0.13024597\n",
      "  0.12491786]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print x, y, w\n",
    "print np.mean(x, axis=0)\n",
    "print np.std(x, axis=0)\n",
    "print np.isfinite(x).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Create a model ####\n",
    "\n",
    "# See https://keras.io/models/about-keras-models/\n",
    "#     https://keras.io/layers/about-keras-layers/\n",
    "# for all kinds of things you can do with Keras\n",
    "\n",
    "# Define model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(8, input_dim=nvariables, activation='tanh', kernel_initializer='glorot_normal'))\n",
    "#model.add(Dense(4, activation='tanh', kernel_initializer='glorot_normal'))\n",
    "#model.add(Dense(1, activation='linear', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Dense(64, input_dim=nvariables, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(0.0000)))\n",
    "#model.add(Dense(32, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(0.0000)))\n",
    "#model.add(Dense(8, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(0.0000)))\n",
    "#model.add(Dense(1, activation='linear', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=nvariables, activation='tanh', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l1_l2(0.00001)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='tanh', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l1_l2(0.00001)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='tanh', kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l1_l2(0.00001)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "# Set loss and optimizer\n",
    "import keras.backend as K\n",
    "def huber_loss(y_true, y_pred, delta=1.345):\n",
    "  x = K.abs(y_true - y_pred)\n",
    "  squared_loss = 0.5*K.square(x)\n",
    "  absolute_loss = delta * (x - 0.5*delta)\n",
    "  xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "  return K.mean(xx, axis=-1)\n",
    "\n",
    "def smooth_huber_loss(y_true, y_pred):\n",
    "  x = K.abs(y_true - y_pred)\n",
    "  return K.mean(x + K.log(1 + K.exp(-2 * x)), axis=-1)\n",
    "\n",
    "def pseudo_huber_loss(y_true, y_pred, delta=1.0):\n",
    "  delta2 = delta * delta\n",
    "  x = delta2 * (K.sqrt(1 + K.square(y_true - y_pred)/delta2) - 1)\n",
    "  return K.mean(x, axis=-1)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "#adam = optimizers.Adam(lr=0.001, decay=0.000001,)\n",
    "#rmsprop = optimizers.RMSprop(lr=0.0001, decay=0.000001,)\n",
    "\n",
    "# Compile\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc', 'mse', 'mae'])\n",
    "#model.compile(loss='mean_squared_error', optimizer=adam, metrics=['acc', 'mse', 'mae'])\n",
    "#model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['acc', 'mse', 'mae'])\n",
    "model.compile(loss=huber_loss, optimizer=adam, metrics=['acc', 'mse', 'mae'])\n",
    "#model.compile(loss=huber_loss, optimizer=rmsprop, metrics=['acc', 'mse', 'mae'])\n",
    "#model.compile(loss='logcosh', optimizer='adam', metrics=['acc', 'mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Training ####\n",
    "\n",
    "#history = model.fit(x_train, y_train, epochs=400, validation_split=0.2, batch_size=1000, verbose=1)\n",
    "\n",
    "# The webpage can become very unresponsive when you train with a large dataset with the above comment.\n",
    "# So I do the following instead.\n",
    "# See issue https://github.com/jupyter/notebook/issues/1474\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = open('keras_output.txt', 'w')\n",
    "history = model.fit(x_train, y_train, epochs=40, validation_split=0.1, batch_size=256, verbose=1)\n",
    "#history = model.fit(x_train, y_train, epochs=200, validation_split=0.1, batch_size=256, verbose=0)\n",
    "#history = model.fit(x_train, y_train, sample_weight=w_train, epochs=40, validation_split=0.1, batch_size=256, verbose=1)\n",
    "sys.stdout.close()\n",
    "sys.stdout = old_stdout\n",
    "print('[INFO] Time elapsed: {0} sec'.format(time.time() - start_time))\n",
    "\n",
    "# Store model to file\n",
    "model.summary()\n",
    "model.save('model.h5')\n",
    "model.save_weights('model_weights.h5')\n",
    "print('[INFO] Model is saved as model.h5 and model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Evaluation using Keras internal tool ####\n",
    "\n",
    "#loss_and_metrics = model.evaluate(x_test, y_test, batch_size=1000)\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, sample_weight=w_test, batch_size=1000)\n",
    "print('[INFO] loss and metrics: {0}'.format(loss_and_metrics))\n",
    "\n",
    "## Accuracy\n",
    "#plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "#plt.title('model accuracy')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Mean Squared Error\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Mean Absolute Error\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Application ####\n",
    "\n",
    "from keras.models import load_model\n",
    "import ROOT\n",
    "\n",
    "# Load model\n",
    "loaded_model = load_model('model.h5', custom_objects={'huber_loss': huber_loss})\n",
    "loaded_model.load_weights('model_weights.h5')\n",
    "\n",
    "ROOT.gROOT.LoadMacro(\"tdrstyle.C\")\n",
    "ROOT.gROOT.ProcessLine(\"setTDRStyle();\")\n",
    "ROOT.gStyle.SetPalette(57)  # kBird\n",
    "ROOT.gStyle.SetPadGridX(True)\n",
    "ROOT.gStyle.SetPadGridY(True)\n",
    "\n",
    "h1 = ROOT.TH1F(\"h1\", \"h1\", 300, -0.3, 0.3)\n",
    "h2a = ROOT.TH2F(\"h2a\", \"h2a\", 100, -0.5, 0.5, 300, -0.3, 0.3)\n",
    "h2b = ROOT.TH2F(\"h2b\", \"h2b\", 100, -0.5, 0.5, 300, -0.3, 0.3)\n",
    "h2c = ROOT.TH2F(\"h2c\", \"h2c\", 100, -0.5, 0.5, 400, -2, 2)\n",
    "h2d = ROOT.TH2F(\"h2d\", \"h2d\", 100, -0.5, 0.5, 400, -2, 2)\n",
    "\n",
    "nentries_test = x_test.shape[0]/5\n",
    "#nentries_test = 100000\n",
    "\n",
    "y_test_meas = loaded_model.predict(x_test[:nentries_test, :])\n",
    "\n",
    "\n",
    "# Loop over events\n",
    "for i in xrange(nentries_test):\n",
    "    y_true = y_test[i]\n",
    "    y_meas = y_test_meas[i]\n",
    "    h1.Fill(y_meas - y_true)\n",
    "    h2a.Fill(y_true, y_meas - y_true) \n",
    "    h2b.Fill(y_true, y_meas)\n",
    "    h2c.Fill(y_true, (y_meas - y_true)/abs(y_true))\n",
    "    h2d.Fill(abs(y_true), (abs(y_meas) - abs(y_true))/abs(y_true)) \n",
    "\n",
    "# Draw\n",
    "c = ROOT.TCanvas()\n",
    "h1.Draw()\n",
    "c.Draw()\n",
    "\n",
    "print h1.GetEntries(), h1.GetMean(), h1.GetRMS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2a.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2b.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2c.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2d.SetStats(0)\n",
    "h2d.SetTitle(\"\")\n",
    "h2d.GetXaxis().SetTitle(\"gen 1/p_{T} [1/GeV]\")\n",
    "h2d.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T}\")\n",
    "h2d.GetXaxis().SetRangeUser(0, 0.5)\n",
    "h2d.GetYaxis().SetRangeUser(-1, 2)\n",
    "h2d.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "\n",
    "hname = \"h2d\"\n",
    "h = h2d.Clone(\"h2d_clone\")\n",
    "h.Draw(\"COLZ\")\n",
    "#gPad.Print(hname+\".png\")\n",
    "#h.RebinX(2)\n",
    "\n",
    "h_pfx = h.ProfileX(hname+\"_pfx\", 1, -1, \"s\")\n",
    "h_pfx.SetMaximum(1.2)\n",
    "h_pfx.SetMinimum(-0.2)\n",
    "h_pfx.Draw()\n",
    "#h_pfx.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "#gPad.Print(h_pfx.GetName()+\".png\")\n",
    "#\n",
    "\n",
    "if True:\n",
    "    # Apply gaussian fits\n",
    "    gr1 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "    gr2 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "    gr1_aspt = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "    gr2_aspt = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "    for i in xrange(h.GetNbinsX()):\n",
    "      h_py = h.ProjectionY(\"_py\", i+1, i+1)\n",
    "      if h_py.Integral() < 15:  continue\n",
    "      #r = h_py.Fit(\"gaus\", \"SNQ\")\n",
    "      r = h_py.Fit(\"gaus\", \"SNQ\", \"\", h_py.GetMean() - 0.04*8, h_py.GetMean() + 0.04*8)\n",
    "      mean, sigma, meanErr, sigmaErr = r.Parameter(1), r.Parameter(2), r.ParError(1), r.ParError(2)\n",
    "      gr1.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "      gr1.SetPointError(i, 0, 0, sigma, sigma)\n",
    "      gr2.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "      gr2.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "      gr1_aspt.SetPoint(i, 1.0/h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "      gr1_aspt.SetPointError(i, 0, 0, sigma, sigma)\n",
    "      gr2_aspt.SetPoint(i, 1.0/h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "      gr2_aspt.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "    #\n",
    "    hname1 = hname\n",
    "    h_pfx = h.ProfileX(hname1+\"_pfx\", 1, -1, \"s\")\n",
    "    h_pfx.Reset()\n",
    "    h_pfx.SetMaximum(1.2)\n",
    "    h_pfx.SetMinimum(-0.2)\n",
    "    h_pfx.Draw()\n",
    "    gr1.Draw(\"p\")\n",
    "    #gr1.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "    #gPad.Print(h_pfx.GetName()+\".png\")\n",
    "    #\n",
    "    hname2 = hname\n",
    "    h_pfx = h.ProfileX(hname2+\"_pfx\", 1, -1, \"s\")\n",
    "    h_pfx.Reset()\n",
    "    h_pfx.SetMaximum(1)\n",
    "    h_pfx.SetMinimum(0)\n",
    "    h_pfx.Draw()\n",
    "    gr2.Draw(\"p\")\n",
    "    #gr2.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "    #gPad.Print(h_pfx.GetName()+\".png\")\n",
    "    #\n",
    "    hname1 = hname\n",
    "    h_pfx = h.ProfileX(hname1+\"_pfx\", 1, -1, \"s\")\n",
    "    h_pfx.Reset()\n",
    "    h_pfx.SetBins(50, 0, 50)\n",
    "    h_pfx.GetXaxis().SetTitle(\"gen p_{T} [GeV]\")\n",
    "    h_pfx.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T} bias\")\n",
    "    h_pfx.SetMaximum(1.2)\n",
    "    h_pfx.SetMinimum(-0.2)\n",
    "    h_pfx.Draw()\n",
    "    gr1_aspt.Draw(\"p\")\n",
    "    #gr1_aspt.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "    #ROOT.gPad.SetLogx(1)\n",
    "    #ROOT.gPad.Print(h_pfx.GetName()+\".png\")\n",
    "    #ROOT.gPad.SetLogx(0)\n",
    "    #\n",
    "    hname2 = hname\n",
    "    h_pfx = h.ProfileX(hname2+\"_pfx\", 1, -1, \"s\")\n",
    "    h_pfx.Reset()\n",
    "    h_pfx.SetStats(0)\n",
    "    h_pfx.SetBins(50, 0, 50)\n",
    "    h_pfx.GetXaxis().SetTitle(\"gen p_{T} [GeV]\")\n",
    "    h_pfx.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T} resolution\")\n",
    "    h_pfx.SetMaximum(1.2)\n",
    "    h_pfx.SetMinimum(-0.2)\n",
    "    #h_pfx.SetMaximum(0.1)\n",
    "    #h_pfx.SetMinimum(-0.01)\n",
    "    h_pfx.Draw()\n",
    "    gr2_aspt.Draw(\"p\")\n",
    "    #gr2_aspt.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "    ROOT.gPad.SetLogx(1)\n",
    "    #ROOT.gPad.Print(h_pfx.GetName()+\".png\")\n",
    "    #ROOT.gPad.SetLogx(0)\n",
    "    \n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "\n",
    "hname = \"h2b\"\n",
    "h = h2b.Clone(\"h2b_clone\")\n",
    "\n",
    "if True:\n",
    "  gr1 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr2 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr3 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  for i in xrange(h.GetNbinsX()):\n",
    "    h_py = h.ProjectionY(\"_py\", i+1, i+1)\n",
    "    if h_py.Integral() < 15:  continue\n",
    "    #r = h_py.Fit(\"gaus\", \"SNQ\")\n",
    "    r = h_py.Fit(\"gaus\", \"SNQ\", \"\", h_py.GetMean() - 0.04*5, h_py.GetMean() + 0.04*5)\n",
    "    mean, sigma, meanErr, sigmaErr = r.Parameter(1), r.Parameter(2), r.ParError(1), r.ParError(2)\n",
    "    #mean, sigma, meanErr, sigmaErr = h_py.GetMean(), h_py.GetRMS(), 0, 0  #FIXME\n",
    "    gr1.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "    gr1.SetPointError(i, 0, 0, sigma, sigma)\n",
    "    gr2.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "    gr2.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "    #print h_py.Integral(), h_py.Integral(h_py.FindBin(mean-sigma), h_py.FindBin(mean+sigma))\n",
    "    loss = 1.0 - h_py.Integral(h_py.FindBin(mean-sigma), h_py.FindBin(mean+sigma)) / h_py.Integral()\n",
    "    gr3.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), loss)\n",
    "    gr3.SetPointError(i, 0, 0, 0, 0)\n",
    "  #\n",
    "  #gr1.Draw(\"ap\")\n",
    "  #gr2.Draw(\"ap\")\n",
    "  gr3.Draw(\"ap\")\n",
    "\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################## PLEASE IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  print x_train.shape, y_train.shape, x_mask_train.shape\n",
    "\n",
    "  fig, axs = plt.subplots(76/4, 4, figsize=(4*4,4*76/4), tight_layout=True)\n",
    "\n",
    "  for i in xrange(x_train.shape[1]):\n",
    "    mask = x_mask_train[...,(i%25)]\n",
    "    mask = mask.astype(np.bool)\n",
    "\n",
    "    #fig, ax = plt.subplots(tight_layout=True)\n",
    "    xmin, xmax = -4, 4\n",
    "    ymin, ymax = -0.2, 0.2\n",
    "    hist = axs[(i/4, i%4)].hist2d(x_train[...,i][~mask], y_train[~mask], bins=40, range=[[xmin, xmax], [ymin, ymax]], cmap=plt.cm.viridis)  #norm=colors.LogNorm(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  df_x = pd.DataFrame(x_train)\n",
    "  df_y = pd.DataFrame(y_train)\n",
    "  df = pd.concat([df_x, df_y], axis = 1)\n",
    "  print df_x.shape, df_y.shape, df.shape\n",
    "  #print df\n",
    "  \n",
    "  df_corr = df.corr()\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.imshow(df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  x_copy  = the_variables.copy()\n",
    "  y_copy  = the_parameters.copy()\n",
    "  x_phi   = x_copy[:, nlayers*0:nlayers*1]\n",
    "  x_theta = x_copy[:, nlayers*1:nlayers*2]\n",
    "  x_bend  = x_copy[:, nlayers*2:nlayers*3]\n",
    "  x_mask  = x_copy[:, nlayers*3:nlayers*4]\n",
    "  x_road  = x_copy[:, nlayers*4:nlayers*5]\n",
    "\n",
    "  #x_phi_median    = x_road[:, 2] * 32 + 16  # multiply by 'quadstrip' unit (4 * 8)\n",
    "  x_phi_median    = x_road[:, 2] * 16 + 8  # multiply by 'doublestrip' unit (2 * 8)\n",
    "  x_phi_median    = x_phi_median[:, np.newaxis]\n",
    "  x_phi          -= x_phi_median\n",
    "\n",
    "  x_theta_median  = np.nanmedian(x_theta, axis=1)\n",
    "  x_theta_median  = x_theta_median[:, np.newaxis]\n",
    "  x_theta        -= x_theta_median\n",
    "\n",
    "  x_copy -= encoder.x_mean\n",
    "  x_copy /= encoder.x_std\n",
    "\n",
    "  df_x = pd.DataFrame(x_copy[:, :nlayers*3])\n",
    "  df_y = pd.DataFrame(y_copy[:, 0])\n",
    "  df = pd.concat([df_x, df_y], axis = 1)\n",
    "  print df_x.shape, df_y.shape, df.shape\n",
    "  #print df\n",
    "  \n",
    "  df_corr = df.corr()\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.imshow(df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  yy = df_corr.iloc[75]\n",
    "  yy = yy ** 2\n",
    "  #print yy\n",
    "\n",
    "  yyy = yy.sort_values(ascending=False)\n",
    "  pd.set_option('display.max_rows',1000)\n",
    "  print yyy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  fig, ax = plt.subplots(tight_layout=True)\n",
    "\n",
    "  i = 50  # ME1/1 bend\n",
    "  mask = x_mask_train[...,(i%25)]\n",
    "  mask = mask.astype(np.bool)\n",
    "\n",
    "  hist = ax.hist(x_train[...,i][~mask], bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  fig, ax = plt.subplots(tight_layout=True)\n",
    "  hist = ax.hist(x_train[...,75] - 1.0, bins=40)\n",
    "  \n",
    "  fig, ax = plt.subplots(tight_layout=True)\n",
    "  hist = ax.hist((encoder.x_theta_median - 3)/83, bins=40)\n",
    "  \n",
    "  print np.mean(x_train[...,75] - 1.0), np.std(x_train[...,75] - 1.0)\n",
    "  print np.mean((encoder.x_theta_median - 3)/83), np.std((encoder.x_theta_median - 3)/83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def l1_loss_f(x):\n",
    "  return np.abs(x)\n",
    "\n",
    "def l2_loss_f(x):\n",
    "  return np.square(x)\n",
    "\n",
    "def huber_loss_f(x, delta=1.345):\n",
    "  x = np.abs(x)\n",
    "  squared_loss = 0.5*np.square(x)\n",
    "  absolute_loss = delta * (x - 0.5*delta)\n",
    "  return np.where(x < delta, squared_loss, absolute_loss)\n",
    "\n",
    "def smooth_huber_loss_f(x):\n",
    "  x = np.abs(x)\n",
    "  return x + np.log(1 + np.exp(-2 * x))\n",
    "\n",
    "def pseudo_huber_loss_f(x, delta=2.):\n",
    "  delta2 = delta*delta\n",
    "  return delta2 * (np.sqrt(1 + np.square(x)/delta2) - 1)\n",
    "\n",
    "if False:\n",
    "  loss_x = np.linspace(-5., 5., 1000)\n",
    "  l1_loss_y = np.apply_along_axis(l1_loss_f, -1, loss_x)\n",
    "  l2_loss_y = np.apply_along_axis(l2_loss_f, -1, loss_x)\n",
    "  huber_loss_y = np.apply_along_axis(huber_loss_f, -1, loss_x)\n",
    "  smooth_huber_loss_y = np.apply_along_axis(smooth_huber_loss_f, -1, loss_x)\n",
    "  pseudo_huber_loss_y = np.apply_along_axis(pseudo_huber_loss_f, -1, loss_x)\n",
    "\n",
    "  plt.plot(loss_x, l2_loss_y, label='L2')\n",
    "  plt.plot(loss_x, l1_loss_y, label='L1')\n",
    "  plt.plot(loss_x, huber_loss_y, label='huber')\n",
    "  #plt.plot(loss_x, smooth_huber_loss_y, label='smooth huber')\n",
    "  plt.plot(loss_x, pseudo_huber_loss_y, label='pseudo huber')\n",
    "  plt.ylim(0,6)\n",
    "  plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  from keras.models import load_model\n",
    "  loaded_model = load_model('model.8.h5', custom_objects={'huber_loss': huber_loss})\n",
    "  loaded_model.load_weights('model_weights.8.h5')\n",
    "\n",
    "  maxEvents = 100\n",
    "  x_check = x[:maxEvents]\n",
    "  y_check = y[:maxEvents]\n",
    "\n",
    "  y_pred_check = loaded_model.predict(x_check)\n",
    "  y_pred_check = y_pred_check[:,0]\n",
    "\n",
    "  for i in xrange(maxEvents):\n",
    "    print i\n",
    "    print y_check[i], y_pred_check[i]\n",
    "    print x_check[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
