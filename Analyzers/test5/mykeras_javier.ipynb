{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using numpy 1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using keras 2.0.5\n",
      "[INFO] Using tensorflow 1.1.0\n",
      "[INFO] Using sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "import random\n",
    "random.seed(2023)\n",
    "print('[INFO] Using numpy {0}'.format(np.__version__))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import initializers, regularizers, optimizers, losses\n",
    "K.set_epsilon(1e-08)\n",
    "print('[INFO] Using keras {0}'.format(keras.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print('[INFO] Using tensorflow {0}'.format(tf.__version__))\n",
    "\n",
    "import sklearn\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('[INFO] Using sklearn {0}'.format(sklearn.__version__))\n",
    "\n",
    "#import pandas as pd\n",
    "#import statsmodels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Globals\n",
    "nlayers = 12  # 5 (CSC) + 4 (RPC) + 3 (GEM)\n",
    "\n",
    "#infile = 'histos_tba.12.npz'\n",
    "infile = '/tmp/jiafu/histos_tba.12.npz'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded the variables with shape (3646057, 76)\n",
      "[INFO] Loaded the parameters with shape (3646057, 3)\n"
     ]
    }
   ],
   "source": [
    "#### Load data ####\n",
    "try:\n",
    "    loaded = np.load(infile)\n",
    "    the_variables = loaded['variables']\n",
    "    the_parameters = loaded['parameters']\n",
    "except:\n",
    "    print('[ERROR] Failed to load data from file: {0}'.format(infile))\n",
    "\n",
    "print('[INFO] Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "print('[INFO] Loaded the parameters with shape {0}'.format(the_parameters.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(object):\n",
    "\n",
    "  def __init__(self, x, y, adjust_scale=0):\n",
    "    if x is not None and y is not None:\n",
    "      assert(x.shape[1] == (nlayers * 6) + 4)\n",
    "      assert(y.shape[1] == 3)\n",
    "      assert(x.shape[0] == y.shape[0])\n",
    "\n",
    "      self.nentries = x.shape[0]\n",
    "      self.x_orig  = x\n",
    "      self.y_orig  = y\n",
    "      self.x_copy  = x.copy()\n",
    "      self.y_copy  = y.copy()\n",
    "\n",
    "      # Get views\n",
    "      self.x_phi   = self.x_copy[:, nlayers*0:nlayers*1]\n",
    "      self.x_theta = self.x_copy[:, nlayers*1:nlayers*2]\n",
    "      self.x_bend  = self.x_copy[:, nlayers*2:nlayers*3]\n",
    "      self.x_ring  = self.x_copy[:, nlayers*3:nlayers*4]\n",
    "      self.x_fr    = self.x_copy[:, nlayers*4:nlayers*5]\n",
    "      self.x_mask  = self.x_copy[:, nlayers*5:nlayers*6].astype(np.bool)  # this makes a copy\n",
    "      self.x_road  = self.x_copy[:, nlayers*6:nlayers*7]  # ipt, ieta, iphi, iphi_corr\n",
    "      self.y_pt    = self.y_copy[:, 0]  # q/pT\n",
    "      self.y_phi   = self.y_copy[:, 1]\n",
    "      self.y_eta   = self.y_copy[:, 2]\n",
    "      \n",
    "      # Make event weight\n",
    "      #self.w       = np.ones(self.y_pt.shape, dtype=np.float32)\n",
    "      self.w       = np.abs(self.y_pt)/0.2 + 1.0\n",
    "      \n",
    "      # Straightness & zone\n",
    "      self.x_straightness = self.x_road[:, 0][:, np.newaxis]\n",
    "      self.x_zone         = self.x_road[:, 1][:, np.newaxis]\n",
    "      \n",
    "      # Subtract median phi from hit phis\n",
    "      #self.x_phi_median    = self.x_road[:, 2] * 32 - 16  # multiply by 'quadstrip' unit (4 * 8)\n",
    "      self.x_phi_median    = self.x_road[:, 2] * 16 - 8  # multiply by 'doublestrip' unit (2 * 8)\n",
    "      self.x_phi_median    = self.x_phi_median[:, np.newaxis]\n",
    "      self.x_phi          -= self.x_phi_median\n",
    "      \n",
    "      # Subtract median theta from hit thetas\n",
    "      self.x_theta_median  = np.nanmedian(self.x_theta[:,:5], axis=1)  # CSC only\n",
    "      self.x_theta_median[np.isnan(self.x_theta_median)] = np.nanmedian(self.x_theta[np.isnan(self.x_theta_median)], axis=1)  # use all\n",
    "      self.x_theta_median  = self.x_theta_median[:, np.newaxis]\n",
    "      self.x_theta        -= self.x_theta_median\n",
    "      \n",
    "      # Standard scales\n",
    "      if adjust_scale == 0:  # do not adjust\n",
    "        pass\n",
    "      elif adjust_scale == 1:  # use mean and std\n",
    "        self.x_mean  = np.nanmean(self.x_copy, axis=0)\n",
    "        self.x_std   = np.nanstd(self.x_copy, axis=0)\n",
    "        self.x_std   = self._handle_zero_in_scale(self.x_std)\n",
    "        self.x_copy -= self.x_mean\n",
    "        self.x_copy /= self.x_std\n",
    "      elif adjust_scale == 2:  # adjust by hand\n",
    "        self.x_phi   *= 0.000991  # GE1/1 dphi linear correlation with q/pT\n",
    "        self.x_theta *= (1/12.)   # 12 integer theta units\n",
    "        self.x_bend  *= 0.188082  # ME1/2 bend linear correlation with q/pT\n",
    "        x_ring_tmp    = self.x_ring.astype(np.int32)\n",
    "        x_ring_tmp    = (x_ring_tmp == 1) | (x_ring_tmp == 4)\n",
    "        self.x_ring[x_ring_tmp] = 0  # ring 1,4 -> 0\n",
    "        self.x_ring[~x_ring_tmp] = 1 # ring 2,3 -> 1\n",
    "        #self.x_fr     = self.x_fr\n",
    "      \n",
    "      # Remove outlier hits by checking hit thetas\n",
    "      if adjust_scale == 0:  # do not adjust\n",
    "        x_theta_tmp = np.abs(self.x_theta) > 10000.0\n",
    "      elif adjust_scale == 1:  # use mean and std\n",
    "        x_theta_tmp = np.abs(self.x_theta) > 1.0\n",
    "      elif adjust_scale == 2:  # adjust by hand\n",
    "        theta_cuts    = np.array((6., 6., 6., 6., 6., 12., 12., 12., 12., 9., 9., 9.), dtype=np.float32)\n",
    "        theta_cuts   *= (1/12.)   # 12 integer theta units\n",
    "        assert(len(theta_cuts) == nlayers)\n",
    "        x_theta_tmp = np.abs(self.x_theta) > theta_cuts\n",
    "      self.x_phi  [x_theta_tmp] = np.nan\n",
    "      self.x_theta[x_theta_tmp] = np.nan\n",
    "      self.x_bend [x_theta_tmp] = np.nan\n",
    "      self.x_ring [x_theta_tmp] = np.nan\n",
    "      self.x_fr   [x_theta_tmp] = np.nan\n",
    "      self.x_mask [x_theta_tmp] = 1.0\n",
    "      \n",
    "      # Add variables: straightness, zone, theta_median and mode variables\n",
    "      self.x_straightness -= 6.  # scaled to [-1,1]\n",
    "      self.x_straightness /= 6.\n",
    "      self.x_zone         -= 0.  # scaled to [0,1]\n",
    "      self.x_zone         /= 5.\n",
    "      self.x_theta_median -= 3.  # scaled to [0,1]\n",
    "      self.x_theta_median /= 83.\n",
    "      hits_to_station = np.array((5,1,2,3,4,1,2,3,4,5,2,5), dtype=np.int32)  # '5' denotes ME1/1\n",
    "      assert(len(hits_to_station) == nlayers)\n",
    "      self.x_mode_vars = np.zeros((self.nentries, 5), dtype=np.bool)\n",
    "      self.x_mode_vars[:,0] = np.any(self.x_mask[:,hits_to_station == 5] == 0, axis=1)\n",
    "      self.x_mode_vars[:,1] = np.any(self.x_mask[:,hits_to_station == 1] == 0, axis=1)\n",
    "      self.x_mode_vars[:,2] = np.any(self.x_mask[:,hits_to_station == 2] == 0, axis=1)\n",
    "      self.x_mode_vars[:,3] = np.any(self.x_mask[:,hits_to_station == 3] == 0, axis=1)\n",
    "      self.x_mode_vars[:,4] = np.any(self.x_mask[:,hits_to_station == 4] == 0, axis=1)\n",
    "      \n",
    "      # Remove NaN\n",
    "      #np.nan_to_num(self.x_copy, copy=False)\n",
    "      self.x_copy[np.isnan(self.x_copy)] = 0.0\n",
    "\n",
    "  # Copied from scikit-learn\n",
    "  def _handle_zero_in_scale(self, scale):\n",
    "    scale[scale == 0.0] = 1.0\n",
    "    return scale\n",
    "\n",
    "  def get_x(self):\n",
    "    #x_new = self.x_phi\n",
    "    x_new = np.hstack((self.x_phi, self.x_theta, self.x_bend, self.x_ring, self.x_fr, self.x_straightness, self.x_zone, self.x_theta_median, self.x_mode_vars))\n",
    "    return x_new\n",
    "\n",
    "  def get_x_mask(self):\n",
    "    x_mask = self.x_mask.copy()\n",
    "    return x_mask\n",
    "\n",
    "  def get_y(self):\n",
    "    y_new = self.y_pt.copy()\n",
    "    return y_new\n",
    "\n",
    "  def get_w(self):\n",
    "    w_new = self.w.copy()\n",
    "    return w_new\n",
    "\n",
    "  def save_encoder(self, filepath):\n",
    "    np.savez_compressed(filepath, x_mean=self.x_mean, x_std=self.x_std)\n",
    "\n",
    "  def load_endcoder(self, filepath):\n",
    "    loaded = np.load(filepath)\n",
    "    self.x_mean = loaded['x_mean']\n",
    "    self.x_std = loaded['x_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-numpy/1.12.1-mlhled2/lib/python2.7/site-packages/numpy-1.12.1-py2.7-linux-x86_64.egg/numpy/lib/function_base.py:3858: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n",
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs/5.0-ghjeda6/lib/python2.7/site-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using 68 variables and 1 parameters\n"
     ]
    }
   ],
   "source": [
    "#### Prepare data ####\n",
    "\n",
    "# Preprocess data\n",
    "encoder = Encoder(the_variables, the_parameters, adjust_scale=2)\n",
    "x, y, w, x_mask = encoder.get_x(), encoder.get_y(), encoder.get_w(), encoder.get_x_mask()\n",
    "#encoder.save_encoder('encoder.npz')\n",
    "#print('[INFO] Encoder is saved as encoder.npz')\n",
    "\n",
    "# Split dataset in training and testing\n",
    "x_train, x_test, y_train, y_test, w_train, w_test, x_mask_train, x_mask_test = train_test_split(x, y, w, x_mask, test_size=0.4)\n",
    "\n",
    "nvariables = x_train.shape[1]\n",
    "nparameters = 1\n",
    "print('[INFO] Using {0} variables and {1} parameters'.format(nvariables, nparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22594799  0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.          0.026757   -0.011892   ...,  1.          1.          1.        ]\n",
      " [ 0.          0.007928    0.003964   ...,  1.          1.          1.        ]\n",
      " ..., \n",
      " [ 0.31216499  0.          0.         ...,  1.          1.          1.        ]\n",
      " [ 0.095136    0.         -0.031712   ...,  1.          1.          1.        ]\n",
      " [ 0.15955099  0.         -0.055496   ...,  1.          1.          1.        ]] [ 0.45731288  0.14871465  0.01238597 ...,  0.37654495  0.13762507\n",
      "  0.43144876] [ 3.28656435  1.74357319  1.06192982 ...,  2.88272476  1.68812537\n",
      "  3.15724373]\n",
      "[ -1.13775465e-03  -1.21311634e-03  -2.28543929e-03  -2.45982758e-03\n",
      "  -2.64636218e-03  -1.21401867e-03  -8.58638552e-04  -2.24672235e-03\n",
      "  -2.14490993e-03  -8.11390462e-04  -9.54931311e-04   0.00000000e+00\n",
      "   7.06632361e-02   2.37259362e-02   4.39805817e-03  -2.49726456e-02\n",
      "  -2.73069106e-02   1.42409028e-02   1.46502145e-02  -2.97896825e-02\n",
      "  -1.15048420e-02   4.02814262e-02   3.34559605e-02   0.00000000e+00\n",
      "  -9.51220645e-05   3.62900493e-04  -5.57257794e-04  -3.61771265e-04\n",
      "   1.03584192e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   2.83089370e-01   9.94553268e-01   3.12793523e-01   4.24619257e-01\n",
      "   5.10615468e-01   9.96653914e-01   9.99981642e-01   4.77105260e-01\n",
      "   5.58263361e-01   6.04973793e-01   5.13784647e-01   1.00000000e+00\n",
      "   3.46161634e-01   1.05925664e-01   4.38623965e-01   4.21928138e-01\n",
      "   4.09876764e-01   1.04877129e-01   9.21831429e-02   5.13031185e-01\n",
      "   4.78214413e-01   2.03694299e-01   2.43201897e-01   0.00000000e+00\n",
      "  -1.32663483e-02   4.95076537e-01   3.73509943e-01   7.23380625e-01\n",
      "   2.58685470e-01   9.41014111e-01   9.79165733e-01   9.46472883e-01]\n",
      "[ 0.14387727  0.02875937  0.06190405  0.06512458  0.07874002  0.02468514\n",
      "  0.01680352  0.0801121   0.0933707   0.1283929   0.04491244  0.\n",
      "  0.11051976  0.07586288  0.05065319  0.05327265  0.07328065  0.18424776\n",
      "  0.12863368  0.21489766  0.19373153  0.10845656  0.09294292  0.\n",
      "  0.19305736  0.16335341  0.10193802  0.14317256  0.13981935  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.44695312\n",
      "  0.07340281  0.47107118  0.49572322  0.49992234  0.05765124  0.00428665\n",
      "  0.49937034  0.49329761  0.48337314  0.49994951  0.          0.48079205\n",
      "  0.30547041  0.49288732  0.49430105  0.49837559  0.3044675   0.29089493\n",
      "  0.49989021  0.49944794  0.39975885  0.4301067   0.          0.47496116\n",
      "  0.34785315  0.23836826  0.43897122  0.43727899  0.23120111  0.14148192\n",
      "  0.22128549]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print x, y, w\n",
    "print np.mean(x, axis=0)\n",
    "print np.std(x, axis=0)\n",
    "print np.isfinite(x).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                4416      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 7,041\n",
      "Trainable params: 7,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#### Create a model ####\n",
    "\n",
    "# See https://keras.io/models/about-keras-models/\n",
    "#     https://keras.io/layers/about-keras-layers/\n",
    "# for all kinds of things you can do with Keras\n",
    "\n",
    "# Define model\n",
    "#model = Sequential()\n",
    "#model.add(Dense(8, input_dim=nvariables, activation='tanh', kernel_initializer='glorot_normal'))\n",
    "#model.add(Dense(4, activation='tanh', kernel_initializer='glorot_normal'))\n",
    "#model.add(Dense(1, activation='linear', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "#model = Sequential()\n",
    "#model.add(Dense(64, input_dim=nvariables, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(0.0000)))\n",
    "#model.add(Dense(32, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(0.0000)))\n",
    "#model.add(Dense(8, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(0.0000)))\n",
    "#model.add(Dense(1, activation='linear', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "l1Reg = 1e-7\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=nvariables, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(l1Reg), name='dense_1'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(l1Reg), name='dense_2'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(l1Reg), name='dense_3'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l1(l1Reg), name='dense_4'))\n",
    "model.load_weights('model.12.h5')\n",
    "\n",
    "# Set loss and optimizer\n",
    "def huber_loss(y_true, y_pred, delta=1.345):\n",
    "  x = K.abs(y_true - y_pred)\n",
    "  squared_loss = 0.5*K.square(x)\n",
    "  absolute_loss = delta * (x - 0.5*delta)\n",
    "  #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "  xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "  return K.mean(xx, axis=-1)\n",
    "\n",
    "# from https://arogozhnikov.github.io/hep_ml/_modules/hep_ml/nnet.html#smooth_huber_loss\n",
    "def smooth_huber_loss(y_true, y_pred):\n",
    "  x = K.abs(y_true - y_pred)\n",
    "  return K.mean(x + K.log(1 + K.exp(-2 * x)), axis=-1)\n",
    "\n",
    "def pseudo_huber_loss(y_true, y_pred, delta=1.0):\n",
    "  delta2 = delta * delta\n",
    "  x = delta2 * (K.sqrt(1 + K.square(y_true - y_pred)/delta2) - 1)\n",
    "  return K.mean(x, axis=-1)\n",
    "\n",
    "# Learning rate decay by epoch number\n",
    "from keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "def lr_schedule(epoch):\n",
    "  if (epoch % 10) == 0:\n",
    "    lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, lr*0.95)\n",
    "    print(\"lr changed to {}\".format(lr*0.95))\n",
    "  return K.get_value(model.optimizer.lr)\n",
    "\n",
    "lr_decay = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001)\n",
    "#adam = optimizers.Adam(lr=0.005)\n",
    "#adam = optimizers.Adam(lr=0.001, amsgrad=True)\n",
    "#adam = optimizers.Adam(lr=0.0001, decay=0.00001,)\n",
    "#rmsprop = optimizers.RMSprop(lr=0.0001, decay=0.00001,)\n",
    "\n",
    "# Compile\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc', 'mse', 'mae'])\n",
    "#model.compile(loss='mean_squared_error', optimizer=adam, metrics=['acc', 'mse', 'mae'])\n",
    "#model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['acc', 'mse', 'mae'])\n",
    "model.compile(loss=huber_loss, optimizer=adam, metrics=['acc', 'mse', 'mae'])\n",
    "#model.compile(loss=huber_loss, optimizer=rmsprop, metrics=['acc', 'mse', 'mae'])\n",
    "#model.compile(loss='logcosh', optimizer='adam', metrics=['acc', 'mse', 'mae'])\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1968870 samples, validate on 218764 samples\n",
      "lr changed to 5.95385376073e-05\n",
      "Epoch 1/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2225e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00000: val_loss improved from inf to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00000: val_loss improved from inf to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2255e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3730e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 2/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2240e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00001: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00001: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2242e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3721e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 3/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2241e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00002: val_loss did not improve\n",
      "Epoch 00002: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2243e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3810e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 4/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2219e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00003: val_loss did not improve\n",
      "Epoch 00003: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2241e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3744e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 5/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2231e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00004: val_loss did not improve\n",
      "Epoch 00004: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2240e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3758e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 6/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2227e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00005: val_loss did not improve\n",
      "Epoch 00005: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2238e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3791e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 7/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2256e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2242e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3730e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 8/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2218e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2228e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3728e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 9/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2259e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00008: val_loss did not improve\n",
      "Epoch 00008: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2238e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3751e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 10/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2259e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00009: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00009: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2230e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3700e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 5.65616121094e-05\n",
      "Epoch 11/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2211e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00010: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2233e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3845e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 12/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2233e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00011: val_loss did not improve\n",
      "Epoch 00011: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2239e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3712e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 13/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2215e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00012: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00012: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2228e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3690e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 14/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2235e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00013: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2219e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3713e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 15/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2217e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00014: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2212e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3729e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 16/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2233e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00015: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2209e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3816e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 17/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2231e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00016: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2221e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3730e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 18/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2150e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00017: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2218e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3714e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 19/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2168e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00018: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2213e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3696e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 20/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2187e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00019: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00019: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2206e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3656e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 5.37335330591e-05\n",
      "Epoch 21/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2221e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00020: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2204e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3668e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 22/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2186e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00021: val_loss did not improve\n",
      "Epoch 00021: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2204e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3907e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 23/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2228e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00022: val_loss did not improve\n",
      "Epoch 00022: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2208e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3692e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 24/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2208e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00023: val_loss did not improve\n",
      "Epoch 00023: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2197e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3699e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 25/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2212e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00024: val_loss did not improve\n",
      "Epoch 00024: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2199e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3666e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 26/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2188e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00025: val_loss did not improve\n",
      "Epoch 00025: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2192e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3698e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 27/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2180e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00026: val_loss did not improve\n",
      "Epoch 00026: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2198e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3776e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 28/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2196e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00027: val_loss did not improve\n",
      "Epoch 00027: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2198e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3657e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 29/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2146e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00028: val_loss did not improve\n",
      "Epoch 00028: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2199e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3726e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 30/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2203e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00029: val_loss did not improve\n",
      "Epoch 00029: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2200e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3675e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 5.10468560606e-05\n",
      "Epoch 31/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2216e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00030: val_loss did not improve\n",
      "Epoch 00030: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2190e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3706e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 32/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2193e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00031: val_loss did not improve\n",
      "Epoch 00031: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2182e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3667e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 33/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2189e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00032: val_loss did not improve\n",
      "Epoch 00032: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2180e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3702e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 34/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2181e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00033: val_loss did not improve\n",
      "Epoch 00033: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2177e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3684e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 35/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2189e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00034: val_loss did not improve\n",
      "Epoch 00034: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2175e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3717e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 36/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2171e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00035: val_loss did not improve\n",
      "Epoch 00035: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2181e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3656e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 37/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2174e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00036: val_loss did not improve\n",
      "Epoch 00036: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2173e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3675e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 38/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2187e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00037: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00037: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2169e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3654e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 39/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2157e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00038: val_loss did not improve\n",
      "Epoch 00038: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2165e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3663e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 40/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2225e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00039: val_loss did not improve\n",
      "Epoch 00039: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2173e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3659e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 4.84945134303e-05\n",
      "Epoch 41/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2170e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00040: val_loss did not improve\n",
      "Epoch 00040: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2167e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3657e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 42/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2167e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00041: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00041: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2172e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3641e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 43/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2152e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00042: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00042: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2154e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3636e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 44/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2179e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00043: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00043: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2155e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3627e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 45/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2107e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00044: val_loss did not improve\n",
      "Epoch 00044: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2145e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3649e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 46/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2149e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00045: val_loss did not improve\n",
      "Epoch 00045: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2157e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3661e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 47/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2110e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00046: val_loss did not improve\n",
      "Epoch 00046: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2152e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3628e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 48/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2193e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00047: val_loss did not improve\n",
      "Epoch 00047: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2162e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3633e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 49/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2159e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00048: val_loss did not improve\n",
      "Epoch 00048: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2151e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3638e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 50/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2150e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00049: val_loss did not improve\n",
      "Epoch 00049: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2146e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3629e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 4.60697865492e-05\n",
      "Epoch 51/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2167e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00050: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00050: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2141e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3615e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 52/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2159e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00051: val_loss did not improve\n",
      "Epoch 00051: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2143e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3633e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 53/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2134e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00052: val_loss did not improve\n",
      "Epoch 00052: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2146e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3647e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 54/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2161e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00053: val_loss did not improve\n",
      "Epoch 00053: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2146e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3655e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 55/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2112e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00054: val_loss did not improve\n",
      "Epoch 00054: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2142e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3626e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 56/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2126e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00055: val_loss did not improve\n",
      "Epoch 00055: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2141e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3655e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 57/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2119e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00056: val_loss did not improve\n",
      "Epoch 00056: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2143e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3641e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 58/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2129e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00057: val_loss did not improve\n",
      "Epoch 00057: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2139e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3647e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 59/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2161e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00058: val_loss did not improve\n",
      "Epoch 00058: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2137e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3661e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 60/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2148e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00059: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00059: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2130e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3609e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 4.37662954937e-05\n",
      "Epoch 61/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2152e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00060: val_loss did not improve\n",
      "Epoch 00060: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2130e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3650e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 62/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2141e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00061: val_loss did not improve\n",
      "Epoch 00061: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2124e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3648e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 63/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2103e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00062: val_loss did not improve\n",
      "Epoch 00062: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2122e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3611e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 64/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2160e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00063: val_loss did not improve\n",
      "Epoch 00063: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2125e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3609e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 65/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2157e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00064: val_loss did not improve\n",
      "Epoch 00064: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2119e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3612e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 66/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2164e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00065: val_loss did not improve\n",
      "Epoch 00065: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2121e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3629e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 67/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2117e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00066: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00066: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2116e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3599e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 68/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2081e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00067: val_loss did not improve\n",
      "Epoch 00067: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2115e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3613e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 69/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2103e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00068: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00068: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2113e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3589e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 70/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2161e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00069: val_loss did not improve\n",
      "Epoch 00069: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2119e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3591e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 4.15779817558e-05\n",
      "Epoch 71/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2086e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00070: val_loss did not improve\n",
      "Epoch 00070: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2105e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3590e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 72/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2078e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00071: val_loss did not improve\n",
      "Epoch 00071: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2112e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3606e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 73/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2103e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00072: val_loss did not improve\n",
      "Epoch 00072: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2109e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3646e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 74/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2129e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00073: val_loss did not improve\n",
      "Epoch 00073: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2108e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3594e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 75/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2092e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00074: val_loss did not improve\n",
      "Epoch 00074: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2107e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3616e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 76/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2086e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00075: val_loss did not improve\n",
      "Epoch 00075: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2101e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3600e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 77/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2104e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00076: val_loss did not improve\n",
      "Epoch 00076: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2104e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3616e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 78/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2098e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00077: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00077: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2105e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3582e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 79/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2111e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00078: val_loss did not improve\n",
      "Epoch 00078: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2098e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3592e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 80/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2114e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00079: val_loss did not improve\n",
      "Epoch 00079: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2098e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3593e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 3.94990840505e-05\n",
      "Epoch 81/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2126e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00080: val_loss did not improve\n",
      "Epoch 00080: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2098e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3592e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 82/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2092e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00081: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00081: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2091e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3577e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 83/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2106e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00082: val_loss did not improve\n",
      "Epoch 00082: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2090e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3604e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 84/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2072e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00083: val_loss did not improve\n",
      "Epoch 00083: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2088e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3701e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 85/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2093e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00084: val_loss did not improve\n",
      "Epoch 00084: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2096e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3585e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 86/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2099e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00085: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00085: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2086e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3570e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 87/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2124e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00086: val_loss did not improve\n",
      "Epoch 00086: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2081e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3621e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 88/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2067e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00087: val_loss did not improve\n",
      "Epoch 00087: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2083e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3578e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 89/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2106e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00088: val_loss did not improve\n",
      "Epoch 00088: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2091e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3598e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 90/200\n",
      "1966080/1968870 [============================>.] - ETA: 0s - loss: 7.2086e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00089: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00089: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2090e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3570e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 3.75241314032e-05\n",
      "Epoch 91/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2075e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00090: val_loss did not improve\n",
      "Epoch 00090: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2078e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3599e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 92/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2064e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00091: val_loss did not improve\n",
      "Epoch 00091: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2079e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3570e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 93/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2051e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00092: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00092: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2071e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3556e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 94/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2104e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00093: val_loss did not improve\n",
      "Epoch 00093: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2078e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3573e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 95/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2031e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00094: val_loss did not improve\n",
      "Epoch 00094: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2070e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3565e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 96/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2059e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00095: val_loss did not improve\n",
      "Epoch 00095: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2064e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3562e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 97/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2066e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00096: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00096: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2074e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3552e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 98/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2086e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00097: val_loss did not improve\n",
      "Epoch 00097: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2072e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3560e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 99/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2060e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00098: val_loss did not improve\n",
      "Epoch 00098: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2072e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3573e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 100/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2068e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00099: val_loss did not improve\n",
      "Epoch 00099: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2064e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3573e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 3.56479258699e-05\n",
      "Epoch 101/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2051e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00100: val_loss did not improve\n",
      "Epoch 00100: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2065e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3583e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 102/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2067e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00101: val_loss did not improve\n",
      "Epoch 00101: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2060e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3561e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 103/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2067e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00102: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00102: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2057e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3545e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 104/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2077e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00103: val_loss did not improve\n",
      "Epoch 00103: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2068e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3594e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 105/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2054e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00104: val_loss did not improve\n",
      "Epoch 00104: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2057e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3550e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 106/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2024e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00105: val_loss did not improve\n",
      "Epoch 00105: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2059e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3587e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 107/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2046e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00106: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00106: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2055e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3539e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 108/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2072e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00107: val_loss did not improve\n",
      "Epoch 00107: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2055e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3555e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 109/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2082e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00108: val_loss did not improve\n",
      "Epoch 00108: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2049e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3551e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 110/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2059e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00109: val_loss did not improve\n",
      "Epoch 00109: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2048e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3579e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 3.38655287123e-05\n",
      "Epoch 111/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2038e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00110: val_loss did not improve\n",
      "Epoch 00110: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2058e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3545e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 112/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2002e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00111: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00111: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2041e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3527e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 113/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2045e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00112: val_loss did not improve\n",
      "Epoch 00112: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2051e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3540e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 114/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2072e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00113: val_loss did not improve\n",
      "Epoch 00113: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2048e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3553e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 115/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2053e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00114: val_loss did not improve\n",
      "Epoch 00114: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2046e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3563e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 116/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2085e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00115: val_loss did not improve\n",
      "Epoch 00115: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2039e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3543e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 117/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2062e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00116: val_loss did not improve\n",
      "Epoch 00116: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2044e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3568e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 118/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2093e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00117: val_loss did not improve\n",
      "Epoch 00117: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2041e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3548e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 119/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2022e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00118: val_loss did not improve\n",
      "Epoch 00118: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2038e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3531e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 120/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2080e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00119: val_loss did not improve\n",
      "Epoch 00119: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2041e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3557e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 3.21722534864e-05\n",
      "Epoch 121/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2062e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00120: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00120: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2040e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3520e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 122/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2052e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00121: val_loss did not improve\n",
      "Epoch 00121: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2033e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3529e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 123/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2066e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00122: val_loss did not improve\n",
      "Epoch 00122: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2030e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3521e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 124/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2054e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00123: val_loss did not improve\n",
      "Epoch 00123: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2032e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3551e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 125/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2042e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00124: val_loss did not improve\n",
      "Epoch 00124: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2028e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3552e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 126/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2063e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00125: val_loss did not improve\n",
      "Epoch 00125: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2032e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3550e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 127/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2027e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00126: val_loss did not improve\n",
      "Epoch 00126: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2030e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3528e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 128/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2031e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00127: val_loss did not improve\n",
      "Epoch 00127: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2028e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3525e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 129/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2069e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00128: val_loss did not improve\n",
      "Epoch 00128: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2026e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3535e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 130/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1982e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00129: val_loss did not improve\n",
      "Epoch 00129: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2028e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3528e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 3.05636418489e-05\n",
      "Epoch 131/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2016e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00130: val_loss did not improve\n",
      "Epoch 00130: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2021e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3525e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 132/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1981e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00131: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00131: val_loss improved from 0.00074 to 0.00074, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2024e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3515e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 133/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2060e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00132: val_loss did not improve\n",
      "Epoch 00132: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2019e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3530e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 134/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2020e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00133: val_loss did not improve\n",
      "Epoch 00133: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2027e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3618e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 135/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1985e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00134: val_loss did not improve\n",
      "Epoch 00134: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2017e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3521e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 136/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1961e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00135: val_loss did not improve\n",
      "Epoch 00135: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2023e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3521e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 137/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1972e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00136: val_loss improved from 0.00074 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00136: val_loss improved from 0.00074 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2016e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3499e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 138/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2039e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00137: val_loss did not improve\n",
      "Epoch 00137: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2018e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3500e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 139/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2054e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00138: val_loss did not improve\n",
      "Epoch 00138: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2016e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3507e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 140/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2040e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00139: val_loss did not improve\n",
      "Epoch 00139: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2013e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3503e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 2.9035460102e-05\n",
      "Epoch 141/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2005e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00140: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00140: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2011e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3496e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 142/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1995e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00141: val_loss did not improve\n",
      "Epoch 00141: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2009e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3496e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 143/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1960e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00142: val_loss did not improve\n",
      "Epoch 00142: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2009e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3526e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 144/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2028e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00143: val_loss did not improve\n",
      "Epoch 00143: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2006e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3523e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 145/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2027e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00144: val_loss did not improve\n",
      "Epoch 00144: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2010e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3517e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 146/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2033e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00145: val_loss did not improve\n",
      "Epoch 00145: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2001e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3515e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 147/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2019e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00146: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00146: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2006e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3484e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 148/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1972e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00147: val_loss did not improve\n",
      "Epoch 00147: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2002e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3537e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 149/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2021e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00148: val_loss did not improve\n",
      "Epoch 00148: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2004e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3521e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 150/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2002e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00149: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00149: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2004e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3480e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 2.75836870969e-05\n",
      "Epoch 151/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1997e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00150: val_loss did not improve\n",
      "Epoch 00150: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1994e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3495e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 152/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1979e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00151: val_loss did not improve\n",
      "Epoch 00151: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.2000e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3503e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 153/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2053e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00152: val_loss did not improve\n",
      "Epoch 00152: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1999e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3483e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 154/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2011e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00153: val_loss did not improve\n",
      "Epoch 00153: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1999e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3513e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 155/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2042e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00154: val_loss did not improve\n",
      "Epoch 00154: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1995e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3519e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 156/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1969e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00155: val_loss did not improve\n",
      "Epoch 00155: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1994e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3492e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 157/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2000e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00156: val_loss did not improve\n",
      "Epoch 00156: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1989e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3502e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 158/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1937e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00157: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00157: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1993e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3480e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 159/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1982e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00158: val_loss did not improve\n",
      "Epoch 00158: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1991e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3524e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 160/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2013e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00159: val_loss did not improve\n",
      "Epoch 00159: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1994e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3494e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 2.62045021373e-05\n",
      "Epoch 161/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1996e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00160: val_loss did not improve\n",
      "Epoch 00160: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1997e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3496e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 162/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1985e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00161: val_loss did not improve\n",
      "Epoch 00161: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1991e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3530e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 163/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1956e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00162: val_loss did not improve\n",
      "Epoch 00162: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1984e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3482e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 164/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2012e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00163: val_loss did not improve\n",
      "Epoch 00163: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1986e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3496e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 165/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1998e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00164: val_loss did not improve\n",
      "Epoch 00164: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1981e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3484e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 166/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1984e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00165: val_loss did not improve\n",
      "Epoch 00165: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1981e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3499e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 167/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1976e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00166: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00166: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1982e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3468e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 168/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1983e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00167: val_loss did not improve\n",
      "Epoch 00167: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1983e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3486e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 169/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.2019e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00168: val_loss did not improve\n",
      "Epoch 00168: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1983e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3502e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 170/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1971e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00169: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00169: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1978e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3464e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 2.48942763392e-05\n",
      "Epoch 171/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1931e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00170: val_loss did not improve\n",
      "Epoch 00170: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1978e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3486e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 172/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1971e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00171: val_loss did not improve\n",
      "Epoch 00171: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1972e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3480e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 173/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1976e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00172: val_loss did not improve\n",
      "Epoch 00172: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1978e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3475e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 174/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1984e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00173: val_loss did not improve\n",
      "Epoch 00173: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1974e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3468e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 175/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1992e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00174: val_loss did not improve\n",
      "Epoch 00174: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1974e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3475e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 176/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1970e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00175: val_loss did not improve\n",
      "Epoch 00175: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1974e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3483e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 177/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1953e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00176: val_loss did not improve\n",
      "Epoch 00176: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1970e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3504e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 178/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1977e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00177: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00177: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1973e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3464e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 179/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1972e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00178: val_loss did not improve\n",
      "Epoch 00178: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1969e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3480e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 180/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1956e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00179: val_loss did not improve\n",
      "Epoch 00179: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1972e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3468e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 2.3649562263e-05\n",
      "Epoch 181/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1971e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00180: val_loss did not improve\n",
      "Epoch 00180: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1969e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3474e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 182/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1959e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00181: val_loss did not improve\n",
      "Epoch 00181: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1968e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3469e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 183/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1956e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00182: val_loss did not improve\n",
      "Epoch 00182: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1965e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3469e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 184/200\n",
      "1966080/1968870 [============================>.] - ETA: 0s - loss: 7.1966e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00183: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00183: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1964e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3460e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 185/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1967e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00184: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00184: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1964e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3456e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 186/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1958e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00185: val_loss did not improve\n",
      "Epoch 00185: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1960e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3474e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 187/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1907e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00186: val_loss did not improve\n",
      "Epoch 00186: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1961e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3463e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 188/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1989e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00187: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7.h5\n",
      "Epoch 00187: val_loss improved from 0.00073 to 0.00073, saving model to model_l1Reg_1e-7_weights.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1963e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3449e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 189/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1967e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00188: val_loss did not improve\n",
      "Epoch 00188: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1959e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3477e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 190/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1935e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00189: val_loss did not improve\n",
      "Epoch 00189: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1955e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3486e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 2.24670835451e-05\n",
      "Epoch 191/200\n",
      "1966080/1968870 [============================>.] - ETA: 0s - loss: 7.1955e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00190: val_loss did not improve\n",
      "Epoch 00190: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1959e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3450e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 192/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1970e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00191: val_loss did not improve\n",
      "Epoch 00191: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1957e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3451e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 193/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1979e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00192: val_loss did not improve\n",
      "Epoch 00192: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1953e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3500e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 194/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1980e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00193: val_loss did not improve\n",
      "Epoch 00193: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1952e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3489e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 195/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1914e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00194: val_loss did not improve\n",
      "Epoch 00194: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1954e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3463e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 196/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1970e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00195: val_loss did not improve\n",
      "Epoch 00195: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1964e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3465e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 197/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1952e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00196: val_loss did not improve\n",
      "Epoch 00196: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1953e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3460e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 198/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1937e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00197: val_loss did not improve\n",
      "Epoch 00197: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1953e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3461e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 199/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1939e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00198: val_loss did not improve\n",
      "Epoch 00198: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1949e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3452e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 200/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 7.1964e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00199: val_loss did not improve\n",
      "Epoch 00199: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 7.1948e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 7.3455e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "[INFO] Time elapsed: 586.524699926 sec\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                4416      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 7,041\n",
      "Trainable params: 7,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[INFO] Model is saved as model.h5, model.json and model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "#### Training ####\n",
    "\n",
    "#history = model.fit(x_train, y_train, epochs=400, validation_split=0.2, batch_size=1000, verbose=1)\n",
    "\n",
    "# The webpage can become very unresponsive when you train with a large dataset with the above comment.\n",
    "# So I do the following instead.\n",
    "# See issue https://github.com/jupyter/notebook/issues/1474\n",
    "modelbestcheck=ModelCheckpoint(\"model_l1Reg_1e-7.h5\",\n",
    "                                            monitor='val_loss', verbose=1,\n",
    "                                            save_best_only=True)\n",
    "modelbestcheckweights=ModelCheckpoint(\"model_l1Reg_1e-7_weights.h5\",\n",
    "                                            monitor='val_loss', verbose=1,\n",
    "                                            save_best_only=True,save_weights_only=True)\n",
    "\n",
    "start_time = time.time()\n",
    "#old_stdout = sys.stdout\n",
    "#sys.stdout = open('keras_output.txt', 'w')\n",
    "#history = model.fit(x_train, y_train, epochs=40, validation_split=0.1, batch_size=256, verbose=1)\n",
    "#history = model.fit(x_train, y_train, epochs=40, validation_split=0.1, batch_size=256, callbacks=[lr_decay], verbose=1)\n",
    "history = model.fit(x_train, y_train, epochs=200, validation_split=0.1, batch_size=8192, callbacks=[lr_decay,modelbestcheck,modelbestcheckweights], verbose=1)\n",
    "#history = model.fit(x_train, y_train, sample_weight=w_train, epochs=40, validation_split=0.1, batch_size=256, verbose=1)\n",
    "#history = model.fit(x_train, y_train, sample_weight=w_train, epochs=200, validation_split=0.1, batch_size=256, verbose=0)\n",
    "#sys.stdout.close()\n",
    "#sys.stdout = old_stdout\n",
    "print('[INFO] Time elapsed: {0} sec'.format(time.time() - start_time))\n",
    "\n",
    "# Store model to file\n",
    "model.summary()\n",
    "#model.save('model_l1Reg.h5')\n",
    "#model.save_weights('model_l1Reg_weights.h5')\n",
    "\n",
    "# Store model to json\n",
    "import json\n",
    "with open('model.json', 'w') as outfile:\n",
    "  json.dump(model.to_json(), outfile)\n",
    "  \n",
    "print('[INFO] Model is saved as model.h5, model.json and model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                4416      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 7,041\n",
      "Trainable params: 7,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from constraints import *\n",
    "import h5py\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "get_custom_objects().update({\"ZeroSomeWeights\": ZeroSomeWeights})\n",
    "\n",
    "h5f = h5py.File('model_l1Reg_1e-7.pruned_drop_weights.h5')\n",
    "\n",
    "l1Reg = 0\n",
    "\n",
    "modelbestcheck=ModelCheckpoint(\"model_l1Reg_1e-7.pruned.50.h5\",\n",
    "                                            monitor='val_loss', verbose=1,\n",
    "                                            save_best_only=True)\n",
    "modelbestcheckweights=ModelCheckpoint(\"model_l1Reg_1e-7_weights.pruned.50.h5\",\n",
    "                                            monitor='val_loss', verbose=1,\n",
    "                                            save_best_only=True,save_weights_only=True)\n",
    "\n",
    "model_const = Sequential()\n",
    "model_const.add(Dense(64, input_dim=nvariables, activation='tanh', kernel_initializer='glorot_uniform', \n",
    "                      kernel_regularizer=regularizers.l1(l1Reg), name='dense_1',\n",
    "                      kernel_constraint = zero_some_weights(binary_tensor=h5f['dense_1'][()].tolist())))\n",
    "model_const.add(Dense(32, activation='tanh', kernel_initializer='glorot_uniform', \n",
    "                      kernel_regularizer=regularizers.l1(l1Reg), name='dense_2',\n",
    "                      kernel_constraint = zero_some_weights(binary_tensor=h5f['dense_2'][()].tolist())))\n",
    "model_const.add(Dense(16, activation='tanh', kernel_initializer='glorot_uniform', \n",
    "                      kernel_regularizer=regularizers.l1(l1Reg), name='dense_3',\n",
    "                      kernel_constraint = zero_some_weights(binary_tensor=h5f['dense_3'][()].tolist())))\n",
    "model_const.add(Dense(1, activation='linear', kernel_initializer='glorot_uniform', \n",
    "                      kernel_regularizer=regularizers.l1(l1Reg), name='dense_4',                      \n",
    "                      kernel_constraint = zero_some_weights(binary_tensor=h5f['dense_4'][()].tolist())))\n",
    "model_const.load_weights('model_l1Reg_1e-7.pruned.h5')\n",
    "\n",
    "model_const.compile(loss=huber_loss, optimizer=adam, metrics=['acc', 'mse', 'mae'])\n",
    "print model_const.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                4416      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 7,041\n",
      "Trainable params: 7,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1968870 samples, validate on 218764 samples\n",
      "lr changed to 2.13437297134e-05\n",
      "Epoch 1/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9298e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233Epoch 00000: val_loss improved from inf to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00000: val_loss improved from inf to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9283e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233 - val_loss: 6.0642e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 2/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9171e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233Epoch 00001: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00001: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9134e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233 - val_loss: 6.0600e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 3/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9099e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233Epoch 00002: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00002: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9099e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233 - val_loss: 6.0572e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 4/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9076e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233Epoch 00003: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00003: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9084e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233 - val_loss: 6.0552e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 5/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9056e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233Epoch 00004: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00004: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9074e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233 - val_loss: 6.0543e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 6/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9064e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233Epoch 00005: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00005: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9060e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233 - val_loss: 6.0535e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 7/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8999e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00006: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00006: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9050e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233 - val_loss: 6.0530e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 8/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9065e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233Epoch 00007: val_loss did not improve\n",
      "Epoch 00007: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9043e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0532e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 9/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9075e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233Epoch 00008: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00008: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9036e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0233 - val_loss: 6.0527e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 10/200\n",
      "1966080/1968870 [============================>.] - ETA: 0s - loss: 5.9034e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00009: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00009: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9036e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0515e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 2.02765440918e-05\n",
      "Epoch 11/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9037e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00010: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9027e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0520e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 12/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9035e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00011: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00011: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9024e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0512e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 13/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8987e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00012: val_loss did not improve\n",
      "Epoch 00012: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9014e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0524e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 14/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9030e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00013: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00013: val_loss improved from 0.00061 to 0.00061, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9017e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0505e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 15/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9004e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00014: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9011e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0513e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 16/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9040e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00015: val_loss improved from 0.00061 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00015: val_loss improved from 0.00061 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9006e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0494e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 17/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8999e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00016: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9000e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0498e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 18/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8981e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00017: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00017: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.9004e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0484e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 19/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.9007e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00018: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8998e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0496e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 20/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8994e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00019: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8996e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0489e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 1.92627168872e-05\n",
      "Epoch 21/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8996e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00020: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8989e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0499e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 22/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8962e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00021: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00021: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8990e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0478e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 23/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8996e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00022: val_loss did not improve\n",
      "Epoch 00022: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8983e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0493e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 24/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8989e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00023: val_loss did not improve\n",
      "Epoch 00023: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8983e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0491e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 25/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8977e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00024: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00024: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8981e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0474e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 26/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8951e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00025: val_loss did not improve\n",
      "Epoch 00025: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8979e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0474e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 27/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8939e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00026: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00026: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8973e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0472e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 28/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8973e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00027: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00027: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8972e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0468e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 29/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8961e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00028: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00028: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8967e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0468e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 30/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8960e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00029: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00029: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8968e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0463e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 1.8299581734e-05\n",
      "Epoch 31/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8933e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00030: val_loss did not improve\n",
      "Epoch 00030: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8962e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0483e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 32/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8957e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00031: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00031: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8960e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0460e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 33/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8897e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00032: val_loss did not improve\n",
      "Epoch 00032: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8960e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0462e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 34/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8937e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00033: val_loss did not improve\n",
      "Epoch 00033: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8957e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0461e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 35/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8970e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00034: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00034: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8958e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0459e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 36/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8926e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00035: val_loss did not improve\n",
      "Epoch 00035: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8953e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0460e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 37/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8976e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00036: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00036: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8949e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0455e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 38/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8922e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00037: val_loss did not improve\n",
      "Epoch 00037: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8950e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0455e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 39/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8927e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00038: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00038: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8947e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0451e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 40/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8934e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00039: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00039: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8945e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0450e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 1.73846018697e-05\n",
      "Epoch 41/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8965e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00040: val_loss did not improve\n",
      "Epoch 00040: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8945e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0460e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 42/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8945e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00041: val_loss did not improve\n",
      "Epoch 00041: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8943e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0454e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 43/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8945e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00042: val_loss did not improve\n",
      "Epoch 00042: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8942e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0457e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 44/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8955e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00043: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00043: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8937e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0448e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 45/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8950e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00044: val_loss did not improve\n",
      "Epoch 00044: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8933e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0454e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 46/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8940e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00045: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00045: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8932e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0447e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 47/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8937e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00046: val_loss did not improve\n",
      "Epoch 00046: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8932e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0462e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 48/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8909e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00047: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00047: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8930e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0446e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 49/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8965e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00048: val_loss did not improve\n",
      "Epoch 00048: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8930e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0456e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 50/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8939e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00049: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00049: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8925e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0442e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 1.65153718626e-05\n",
      "Epoch 51/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8940e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00050: val_loss did not improve\n",
      "Epoch 00050: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8926e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0448e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 52/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8905e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00051: val_loss did not improve\n",
      "Epoch 00051: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8920e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0445e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 53/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8922e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00052: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00052: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8919e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0440e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 54/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8921e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00053: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00053: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8919e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0434e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 55/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8899e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00054: val_loss did not improve\n",
      "Epoch 00054: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8916e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0437e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 56/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8904e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00055: val_loss did not improve\n",
      "Epoch 00055: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8916e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0436e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 57/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8890e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00056: val_loss did not improve\n",
      "Epoch 00056: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8913e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0442e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 58/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8915e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00057: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00057: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8913e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0430e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 59/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8920e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00058: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00058: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8910e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0426e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 60/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8917e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00059: val_loss did not improve\n",
      "Epoch 00059: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8909e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0428e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 1.56896037879e-05\n",
      "Epoch 61/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8934e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00060: val_loss did not improve\n",
      "Epoch 00060: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8908e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0445e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 62/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8857e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00061: val_loss did not improve\n",
      "Epoch 00061: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8905e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0430e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 63/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8906e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00062: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00062: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8902e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0424e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 64/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8869e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00063: val_loss did not improve\n",
      "Epoch 00063: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8903e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0424e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 65/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8937e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00064: val_loss did not improve\n",
      "Epoch 00064: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8900e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0426e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 66/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8878e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00065: val_loss did not improve\n",
      "Epoch 00065: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8900e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0428e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 67/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8915e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00066: val_loss did not improve\n",
      "Epoch 00066: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8897e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0427e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 68/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8900e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00067: val_loss did not improve\n",
      "Epoch 00067: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8895e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0426e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 69/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8925e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00068: val_loss did not improve\n",
      "Epoch 00068: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8897e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0424e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 70/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8948e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00069: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00069: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8895e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0421e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 1.49051237713e-05\n",
      "Epoch 71/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8930e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00070: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00070: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8894e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0416e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 72/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8912e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00071: val_loss did not improve\n",
      "Epoch 00071: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8891e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0418e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 73/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8923e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00072: val_loss did not improve\n",
      "Epoch 00072: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8892e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0425e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 74/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8853e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00073: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00073: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8886e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0410e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 75/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8876e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00074: val_loss did not improve\n",
      "Epoch 00074: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8887e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0422e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 76/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8854e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00075: val_loss did not improve\n",
      "Epoch 00075: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8886e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0412e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 77/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8891e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00076: val_loss did not improve\n",
      "Epoch 00076: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8883e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0414e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 78/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8907e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00077: val_loss did not improve\n",
      "Epoch 00077: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8882e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0424e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 79/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8884e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00078: val_loss did not improve\n",
      "Epoch 00078: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8883e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0414e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 80/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8883e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00079: val_loss did not improve\n",
      "Epoch 00079: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8877e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0416e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 1.41598676692e-05\n",
      "Epoch 81/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8862e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00080: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00080: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8878e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0408e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 82/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8878e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00081: val_loss did not improve\n",
      "Epoch 00081: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8876e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0422e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 83/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8886e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00082: val_loss did not improve\n",
      "Epoch 00082: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8876e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0412e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 84/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8883e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00083: val_loss did not improve\n",
      "Epoch 00083: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8877e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0426e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 85/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8893e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00084: val_loss did not improve\n",
      "Epoch 00084: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8874e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0414e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 86/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8884e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00085: val_loss did not improve\n",
      "Epoch 00085: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8870e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0409e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 87/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8889e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00086: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00086: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8870e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0402e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 88/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8877e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00087: val_loss did not improve\n",
      "Epoch 00087: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8872e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0407e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 89/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8850e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00088: val_loss did not improve\n",
      "Epoch 00088: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8870e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0408e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 90/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8873e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00089: val_loss did not improve\n",
      "Epoch 00089: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8868e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0411e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 1.34518741561e-05\n",
      "Epoch 91/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8849e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00090: val_loss did not improve\n",
      "Epoch 00090: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8866e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0410e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 92/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8865e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00091: val_loss did not improve\n",
      "Epoch 00091: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8865e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0404e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 93/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8821e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00092: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00092: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8863e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0396e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 94/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8787e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00093: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00093: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8863e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0396e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 95/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8857e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00094: val_loss did not improve\n",
      "Epoch 00094: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8860e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0401e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 96/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8868e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00095: val_loss did not improve\n",
      "Epoch 00095: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8862e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0407e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 97/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8856e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00096: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00096: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8861e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0395e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 98/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8865e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00097: val_loss did not improve\n",
      "Epoch 00097: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8859e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0397e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 99/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8847e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00098: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00098: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8859e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0393e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 100/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8893e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00099: val_loss did not improve\n",
      "Epoch 00099: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8853e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0397e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 1.27792804051e-05\n",
      "Epoch 101/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8853e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00100: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00100: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8854e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0392e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 102/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8812e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00101: val_loss did not improve\n",
      "Epoch 00101: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8854e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0400e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 103/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8855e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00102: val_loss did not improve\n",
      "Epoch 00102: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8855e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0405e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 104/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8863e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00103: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00103: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8852e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0389e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 105/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8829e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00104: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00104: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8849e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0389e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 106/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8826e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00105: val_loss did not improve\n",
      "Epoch 00105: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8848e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0389e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 107/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8852e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00106: val_loss did not improve\n",
      "Epoch 00106: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8846e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0392e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 108/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8814e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00107: val_loss did not improve\n",
      "Epoch 00107: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8847e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0398e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 109/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8914e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00108: val_loss did not improve\n",
      "Epoch 00108: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8846e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0391e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 110/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8857e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00109: val_loss did not improve\n",
      "Epoch 00109: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8846e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0389e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 1.21403160392e-05\n",
      "Epoch 111/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8855e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00110: val_loss did not improve\n",
      "Epoch 00110: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8843e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0393e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 112/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8849e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00111: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00111: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8843e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0380e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 113/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8819e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00112: val_loss did not improve\n",
      "Epoch 00112: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8843e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0387e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 114/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8845e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00113: val_loss did not improve\n",
      "Epoch 00113: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8840e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0390e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 115/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8792e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00114: val_loss did not improve\n",
      "Epoch 00114: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8840e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0397e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 116/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8824e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00115: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00115: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8841e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0379e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 117/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8827e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00116: val_loss did not improve\n",
      "Epoch 00116: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8837e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0396e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 118/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8849e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00117: val_loss did not improve\n",
      "Epoch 00117: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8836e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0381e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 119/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8847e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00118: val_loss did not improve\n",
      "Epoch 00118: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8837e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0388e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 120/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8837e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00119: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00119: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8836e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0377e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 1.15333005397e-05\n",
      "Epoch 121/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8825e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00120: val_loss did not improve\n",
      "Epoch 00120: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8832e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0378e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 122/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8825e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00121: val_loss did not improve\n",
      "Epoch 00121: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8835e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0382e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 123/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8799e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00122: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00122: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8831e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0376e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 124/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8828e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00123: val_loss did not improve\n",
      "Epoch 00123: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8831e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0380e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 125/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8831e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00124: val_loss did not improve\n",
      "Epoch 00124: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8830e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0387e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 126/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8835e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00125: val_loss did not improve\n",
      "Epoch 00125: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8830e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0382e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 127/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8816e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00126: val_loss did not improve\n",
      "Epoch 00126: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8830e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0386e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 128/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8841e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00127: val_loss did not improve\n",
      "Epoch 00127: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8829e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0380e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 129/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8811e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00128: val_loss did not improve\n",
      "Epoch 00128: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8827e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0378e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 130/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8854e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00129: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00129: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8827e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0374e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 1.09566354695e-05\n",
      "Epoch 131/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8843e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00130: val_loss did not improve\n",
      "Epoch 00130: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8824e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0375e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 132/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8793e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00131: val_loss did not improve\n",
      "Epoch 00131: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8824e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0380e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 133/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8767e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00132: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00132: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8826e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0371e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 134/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8836e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00133: val_loss did not improve\n",
      "Epoch 00133: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8823e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0378e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 135/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8847e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00134: val_loss did not improve\n",
      "Epoch 00134: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8821e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0372e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 136/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8820e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00135: val_loss did not improve\n",
      "Epoch 00135: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8821e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0377e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 137/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8816e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00136: val_loss did not improve\n",
      "Epoch 00136: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8820e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0384e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 138/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8828e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00137: val_loss did not improve\n",
      "Epoch 00137: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8819e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0375e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 139/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8772e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00138: val_loss did not improve\n",
      "Epoch 00138: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8819e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0375e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 140/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8849e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00139: val_loss did not improve\n",
      "Epoch 00139: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8817e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0371e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "lr changed to 1.04088036096e-05\n",
      "Epoch 141/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8843e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00140: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00140: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8816e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0369e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 142/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8780e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00141: val_loss did not improve\n",
      "Epoch 00141: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8814e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0376e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 143/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8799e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00142: val_loss did not improve\n",
      "Epoch 00142: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8814e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0374e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 144/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8797e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00143: val_loss did not improve\n",
      "Epoch 00143: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8814e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0370e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 145/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8775e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00144: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00144: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8813e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0367e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 146/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8828e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00145: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00145: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8814e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0367e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 147/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8817e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00146: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00146: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8812e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0366e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 148/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8853e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00147: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00147: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8810e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0365e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 149/200\n",
      "1957888/1968870 [============================>.] - ETA: 0s - loss: 5.8812e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00148: val_loss did not improve\n",
      "Epoch 00148: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8812e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0367e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 150/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8803e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00149: val_loss did not improve\n",
      "Epoch 00149: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8809e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0366e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 9.88836377473e-06\n",
      "Epoch 151/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8818e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00150: val_loss did not improve\n",
      "Epoch 00150: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8806e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0367e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 152/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8820e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00151: val_loss did not improve\n",
      "Epoch 00151: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8806e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0369e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 153/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8762e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00152: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00152: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8808e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0365e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 154/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8807e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00153: val_loss did not improve\n",
      "Epoch 00153: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8806e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0374e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 155/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8815e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00154: val_loss did not improve\n",
      "Epoch 00154: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8804e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0372e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 156/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8821e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00155: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00155: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8804e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0360e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 157/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8818e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00156: val_loss did not improve\n",
      "Epoch 00156: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8802e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0363e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 158/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8786e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00157: val_loss did not improve\n",
      "Epoch 00157: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8804e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0368e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 159/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8814e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00158: val_loss did not improve\n",
      "Epoch 00158: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8802e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0362e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 160/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8791e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00159: val_loss did not improve\n",
      "Epoch 00159: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8800e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0362e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 9.3939456292e-06\n",
      "Epoch 161/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8824e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00160: val_loss did not improve\n",
      "Epoch 00160: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8801e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0364e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 162/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8783e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00161: val_loss did not improve\n",
      "Epoch 00161: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8800e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0371e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 163/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8770e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00162: val_loss did not improve\n",
      "Epoch 00162: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8801e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0366e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 164/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8822e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00163: val_loss did not improve\n",
      "Epoch 00163: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8798e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0361e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 165/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8780e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00164: val_loss did not improve\n",
      "Epoch 00164: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8796e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0363e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 166/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8817e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00165: val_loss did not improve\n",
      "Epoch 00165: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8799e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0360e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 167/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8784e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00166: val_loss did not improve\n",
      "Epoch 00166: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8796e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0362e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 168/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8796e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00167: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00167: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8796e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0359e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 169/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8818e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00168: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00168: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8797e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0355e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 170/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8793e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00169: val_loss did not improve\n",
      "Epoch 00169: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8793e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0362e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 8.92424795893e-06\n",
      "Epoch 171/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8794e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00170: val_loss did not improve\n",
      "Epoch 00170: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8796e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0359e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 172/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8780e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00171: val_loss did not improve\n",
      "Epoch 00171: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8793e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0357e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 173/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8757e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00172: val_loss did not improve\n",
      "Epoch 00172: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8791e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0359e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 174/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8771e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00173: val_loss did not improve\n",
      "Epoch 00173: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8792e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0366e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 175/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8827e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00174: val_loss did not improve\n",
      "Epoch 00174: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8790e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0358e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 176/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8809e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00175: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00175: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8789e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0354e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 177/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8805e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00176: val_loss did not improve\n",
      "Epoch 00176: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8790e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0355e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 178/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8825e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00177: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00177: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8790e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0353e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 179/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8772e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00178: val_loss did not improve\n",
      "Epoch 00178: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8789e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0357e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 180/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8815e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00179: val_loss did not improve\n",
      "Epoch 00179: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8787e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0355e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 8.47803521538e-06\n",
      "Epoch 181/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8793e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00180: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00180: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8786e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0351e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 182/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8811e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00181: val_loss did not improve\n",
      "Epoch 00181: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8786e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0351e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 183/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8785e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00182: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00182: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8787e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0351e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 184/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8763e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00183: val_loss did not improve\n",
      "Epoch 00183: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8784e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0360e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 185/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8817e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00184: val_loss did not improve\n",
      "Epoch 00184: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8785e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0353e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 186/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8817e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00185: val_loss did not improve\n",
      "Epoch 00185: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8785e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0352e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 187/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8776e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00186: val_loss did not improve\n",
      "Epoch 00186: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8783e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0353e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 188/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8811e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00187: val_loss did not improve\n",
      "Epoch 00187: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8783e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0362e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 189/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8811e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00188: val_loss did not improve\n",
      "Epoch 00188: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8783e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0351e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 190/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8808e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00189: val_loss did not improve\n",
      "Epoch 00189: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8781e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0352e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "lr changed to 8.0541331954e-06\n",
      "Epoch 191/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8796e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00190: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00190: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8781e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0350e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 192/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8772e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00191: val_loss did not improve\n",
      "Epoch 00191: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8780e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0358e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 193/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8769e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00192: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00192: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8781e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0346e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 194/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8743e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00193: val_loss did not improve\n",
      "Epoch 00193: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8778e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0350e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 195/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8791e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00194: val_loss did not improve\n",
      "Epoch 00194: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8779e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0348e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 196/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8762e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00195: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7.pruned.50.h5\n",
      "Epoch 00195: val_loss improved from 0.00060 to 0.00060, saving model to model_l1Reg_1e-7_weights.pruned.50.h5\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8776e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0346e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 197/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8795e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00196: val_loss did not improve\n",
      "Epoch 00196: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8777e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0348e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 198/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8788e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00197: val_loss did not improve\n",
      "Epoch 00197: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8778e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0346e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0233\n",
      "Epoch 199/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8767e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00198: val_loss did not improve\n",
      "Epoch 00198: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8777e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0352e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n",
      "Epoch 200/200\n",
      "1933312/1968870 [============================>.] - ETA: 0s - loss: 5.8777e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232Epoch 00199: val_loss did not improve\n",
      "Epoch 00199: val_loss did not improve\n",
      "1968870/1968870 [==============================] - 2s - loss: 5.8776e-04 - acc: 0.0000e+00 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0232 - val_loss: 6.0357e-04 - val_acc: 0.0000e+00 - val_mean_squared_error: 0.0012 - val_mean_absolute_error: 0.0234\n"
     ]
    }
   ],
   "source": [
    "history = model_const.fit(x_train, y_train, epochs=200, validation_split=0.1, batch_size=8192, callbacks=[lr_decay,modelbestcheck,modelbestcheckweights], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1449000/1458423 [============================>.] - ETA: 0s[INFO] loss and metrics: [0.00060165848258316628, 0.0, 0.0012033171055745345, 0.023317360071388436]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAasAAAEWCAYAAADYRbjGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n41msJEmNZ7GSJDXeG+LCwLfybc2TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3678129e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEWCAYAAADGjIh1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlJxzzmUMT0rOOecyhicl55xzGeP/BwRafkhAmKgyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3676dce290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEWCAYAAABWn/G6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzjnnYuWJxznnXKw88TjnnIvV/wOaQyZwWeKofQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3677a3c6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Evaluation using Keras internal tool ####\n",
    "\n",
    "loss_and_metrics = model_const.evaluate(x_test, y_test, batch_size=1000)\n",
    "#loss_and_metrics = model.evaluate(x_test, y_test, sample_weight=w_test, batch_size=1000)\n",
    "print('[INFO] loss and metrics: {0}'.format(loss_and_metrics))\n",
    "\n",
    "## Accuracy\n",
    "#plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "#plt.title('model accuracy')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Mean Squared Error\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error'])\n",
    "plt.title('model mse')\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Mean Absolute Error\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model mae')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.constraints import *\n",
    "from keras.layers.merge import multiply\n",
    "\n",
    "class ZeroSomeWeights(Constraint):\n",
    "    \"\"\"ZeroSomeWeights weight constraint.\n",
    "    Constrains certain weights incident to each hidden unit\n",
    "    to be zero.\n",
    "    # Arguments\n",
    "        binary_tensor: binary tensor of 0 or 1s corresponding to which weights to zero.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, binary_tensor=None):\n",
    "        self.binary_tensor = binary_tensor\n",
    "\n",
    "    def __call__(self, w):\n",
    "        if self.binary_tensor is not None:\n",
    "            w = multiply([w,  K.variable(value=self.binary_tensor)])\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'binary_tensor': self.binary_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.         -0.88222176 ...,  0.24093209  0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.         -9.01787949  0.         ...,  2.37122679  0.          1.8836503 ]\n",
      " ..., \n",
      " [ 0.         -0.07171812  0.04123914 ...,  0.09095968  0.         -0.47919241]\n",
      " [ 0.          0.13986062 -0.02667909 ...,  0.          0.          0.09667966]\n",
      " [ 0.          0.08570945 -0.18265548 ..., -0.08766235  0.          0.74228483]]\n"
     ]
    }
   ],
   "source": [
    "#### Application ####\n",
    "\n",
    "from keras.models import load_model, model_from_json\n",
    "import json\n",
    "import ROOT\n",
    "\n",
    "# Load model\n",
    "#loaded_model = load_model('model_l1Reg_1e-7.h5', custom_objects={'huber_loss': huber_loss})\n",
    "#loaded_model.load_weights('model_l1Reg_1e-7_weights.h5')\n",
    "\n",
    "model_file = '/tmp/jiafu/model.12.pruned.50_again.json'\n",
    "model_weights_file = '/tmp/jiafu/model.12.pruned.50_again_weights.h5'\n",
    "with open(model_file) as f:\n",
    "  json_string = json.dumps(json.load(f))\n",
    "  loaded_model = model_from_json(json_string, custom_objects={'huber_loss': huber_loss})\n",
    "  loaded_model.load_weights(model_weights_file)\n",
    "\n",
    "#loaded_model = load_model('model.12.h5', custom_objects={'huber_loss': huber_loss, 'ZeroSomeWeights':ZeroSomeWeights})\n",
    "#print(loaded_model.layers[0].get_weights()[0])\n",
    "loaded_model.load_weights('model_l1Reg_1e-7.pruned.50_again.h5')\n",
    "print(loaded_model.layers[0].get_weights()[0])\n",
    "\n",
    "#loaded_model = load_model('model.12.h5', custom_objects={'huber_loss': huber_loss})\n",
    "#loaded_model.load_weights('model.12.h5')\n",
    "\n",
    "#loaded_model.load_weights('model_l1Reg_1e-7.pruned.h5')\n",
    "#loaded_model.load_weights('model.12.pruned.25_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243070.0 0.000156424708574 0.0334876734684\n",
      "243070.0 0.000349964811674 0.146676318715\n",
      " FCN=254.137 FROM HESSE     STATUS=OK             16 CALLS          95 TOTAL\n",
      "                     EDM=3.2382e-10    STRATEGY= 1      ERROR MATRIX ACCURATE \n",
      "  EXT PARAMETER                                   STEP         FIRST   \n",
      "  NO.   NAME      VALUE            ERROR          SIZE      DERIVATIVE \n",
      "   1  Constant     2.57269e+03   7.75293e+00   1.28217e-02  -2.12744e-06\n",
      "   2  Mean         5.98762e-05   2.88610e-04   6.36034e-07   4.50677e-02\n",
      "   3  Sigma        1.07985e-01   3.00438e-04   1.68444e-06  -1.19141e-02\n",
      "                               ERR DEF= 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TROOT::Append>: Replacing existing TH1: h1 (Potential memory leak).\n",
      "Warning in <TROOT::Append>: Replacing existing TH1: h2 (Potential memory leak).\n",
      "Warning in <TROOT::Append>: Replacing existing TH1: h2a (Potential memory leak).\n",
      "Warning in <TROOT::Append>: Replacing existing TH1: h2b (Potential memory leak).\n",
      "Warning in <TROOT::Append>: Replacing existing TH1: h2c (Potential memory leak).\n",
      "Warning in <TROOT::Append>: Replacing existing TH1: h2d (Potential memory leak).\n",
      "Info in <TCanvas::Print>: pdf file h1_1e-7_pruned50_final.pdf has been created\n",
      "Info in <TCanvas::Print>: pdf file h2_1e-7_pruned50_final.pdf has been created\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAI8CAIAAAC4XaJJAAAABmJLR0QAAAAAAAD5Q7t/AAAgAElE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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROOT.gROOT.LoadMacro(\"tdrstyle.C\")\n",
    "ROOT.gROOT.ProcessLine(\"setTDRStyle();\")\n",
    "ROOT.gStyle.SetPalette(57)  # kBird\n",
    "ROOT.gStyle.SetPadGridX(True)\n",
    "ROOT.gStyle.SetPadGridY(True)\n",
    "\n",
    "h1 = ROOT.TH1F(\"h1\", \"h1\", 300, -0.3, 0.3)\n",
    "h2 = ROOT.TH1F(\"h2\", \"h2\", 300, -0.5, 0.5)\n",
    "h2a = ROOT.TH2F(\"h2a\", \"h2a\", 100, -0.5, 0.5, 300, -0.3, 0.3)\n",
    "h2b = ROOT.TH2F(\"h2b\", \"h2b\", 100, -0.5, 0.5, 300, -0.5, 0.5)\n",
    "h2c = ROOT.TH2F(\"h2c\", \"h2c\", 100, -0.5, 0.5, 400, -2, 2)\n",
    "h2d = ROOT.TH2F(\"h2d\", \"h2d\", 100, -0.5, 0.5, 400, -2, 2)\n",
    "\n",
    "nentries_test = x_test.shape[0]/6\n",
    "#nentries_test = 100000\n",
    "\n",
    "y_test_meas = loaded_model.predict(x_test[:nentries_test, :])\n",
    "\n",
    "\n",
    "# Loop over events\n",
    "for i in xrange(nentries_test):\n",
    "  y_true = y_test[i]\n",
    "  y_meas = y_test_meas[i]\n",
    "  h1.Fill(y_meas - y_true)\n",
    "  h2.Fill((y_meas - y_true)/abs(y_true))\n",
    "  h2a.Fill(y_true, y_meas - y_true) \n",
    "  h2b.Fill(y_true, y_meas)\n",
    "  h2c.Fill(y_true, (y_meas - y_true)/abs(y_true))\n",
    "  h2d.Fill(abs(y_true), (abs(y_meas) - abs(y_true))/abs(y_true)) \n",
    "\n",
    "\n",
    "# Draw\n",
    "c = ROOT.TCanvas()\n",
    "h1.Draw()\n",
    "c.Draw()\n",
    "c.SaveAs('h1_1e-7_pruned50_final.pdf')\n",
    "#c.SaveAs('h1_1e-7.pdf')\n",
    "h2.SetMaximum(3000)\n",
    "h2.Draw()\n",
    "h2.Fit('gaus',\"ml\",\"\",-0.2,0.2)\n",
    "h2.Draw('same')\n",
    "h2.GetXaxis().SetTitle('#Delta p_{T}/p_{T}')\n",
    "c.Draw()\n",
    "c.SaveAs('h2_1e-7_pruned50_final.pdf')\n",
    "#c.SaveAs('h2_1e-7.pdf')\n",
    "print h1.GetEntries(), h1.GetMean(), h1.GetRMS()\n",
    "print h2.GetEntries(), h2.GetMean(), h2.GetRMS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2a.Draw(\"COLZ\")\n",
    "c.Draw()\n",
    "c.SaveAs('h2a.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2b.Draw(\"COLZ\")\n",
    "c.Draw()\n",
    "c.SaveAs('h2b.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2c.Draw(\"COLZ\")\n",
    "c.Draw()\n",
    "c.SaveAs('h2c.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2d.SetStats(0)\n",
    "h2d.SetTitle(\"\")\n",
    "h2d.GetXaxis().SetTitle(\"gen 1/p_{T} [1/GeV]\")\n",
    "h2d.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T}\")\n",
    "h2d.GetXaxis().SetRangeUser(0, 0.5)\n",
    "h2d.GetYaxis().SetRangeUser(-1, 2)\n",
    "h2d.Draw(\"COLZ\")\n",
    "c.Draw()\n",
    "c.SaveAs('h2d.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAI8CAIAAAC4XaJJAAAABmJLR0QAAAAAAAD5Q7t/AAAgAElE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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = ROOT.TCanvas()\n",
    "\n",
    "hname = \"h2d\"\n",
    "h = h2d.Clone(\"h2d_clone\")\n",
    "h.Draw(\"COLZ\")\n",
    "#gPad.Print(hname+\".png\")\n",
    "#h.RebinX(2)\n",
    "\n",
    "h_pfx = h.ProfileX(hname+\"_pfx\", 1, -1, \"s\")\n",
    "h_pfx.SetMaximum(1.2)\n",
    "h_pfx.SetMinimum(-0.2)\n",
    "h_pfx.Draw()\n",
    "#h_pfx.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "#gPad.Print(h_pfx.GetName()+\".png\")\n",
    "#\n",
    "\n",
    "if True:\n",
    "  # Apply gaussian fits\n",
    "  gr1 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr2 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr1_aspt = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr2_aspt = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  for i in xrange(h.GetNbinsX()):\n",
    "    h_py = h.ProjectionY(\"_py\", i+1, i+1)\n",
    "    if h_py.Integral() < 15:  continue\n",
    "    #r = h_py.Fit(\"gaus\", \"SNQ\")\n",
    "    r = h_py.Fit(\"gaus\", \"SNQ\", \"\", h_py.GetMean() - 0.04*8, h_py.GetMean() + 0.04*8)\n",
    "    mean, sigma, meanErr, sigmaErr = r.Parameter(1), r.Parameter(2), r.ParError(1), r.ParError(2)\n",
    "    gr1.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "    gr1.SetPointError(i, 0, 0, sigma, sigma)\n",
    "    gr2.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "    gr2.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "    gr1_aspt.SetPoint(i, 1.0/h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "    gr1_aspt.SetPointError(i, 0, 0, sigma, sigma)\n",
    "    gr2_aspt.SetPoint(i, 1.0/h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "    gr2_aspt.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "  #\n",
    "  hname1 = hname\n",
    "  h_pfx = h.ProfileX(hname1+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  h_pfx.Draw()\n",
    "  gr1.Draw(\"p\")\n",
    "  #gr1.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #\n",
    "  hname2 = hname\n",
    "  h_pfx = h.ProfileX(hname2+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetMaximum(1)\n",
    "  h_pfx.SetMinimum(0)\n",
    "  h_pfx.Draw()\n",
    "  gr2.Draw(\"p\")\n",
    "  #gr2.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #\n",
    "  hname1 = hname\n",
    "  h_pfx = h.ProfileX(hname1+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetBins(50, 0, 50)\n",
    "  h_pfx.GetXaxis().SetTitle(\"gen p_{T} [GeV]\")\n",
    "  h_pfx.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T} bias\")\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  h_pfx.Draw()\n",
    "  gr1_aspt.Draw(\"p\")\n",
    "  #gr1_aspt.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #ROOT.gPad.SetLogx(1)\n",
    "  #ROOT.gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #ROOT.gPad.SetLogx(0)\n",
    "  #\n",
    "  hname2 = hname\n",
    "  h_pfx = h.ProfileX(hname2+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetStats(0)\n",
    "  h_pfx.SetBins(50, 0, 50)\n",
    "  h_pfx.GetXaxis().SetTitle(\"gen p_{T} [GeV]\")\n",
    "  h_pfx.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T} resolution\")\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  #h_pfx.SetMaximum(0.1)\n",
    "  #h_pfx.SetMinimum(-0.01)\n",
    "  h_pfx.Draw()\n",
    "  gr2_aspt.Draw(\"p\")\n",
    "  #gr2_aspt.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  ROOT.gPad.SetLogx(1)\n",
    "  #ROOT.gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #ROOT.gPad.SetLogx(0)\n",
    "    \n",
    "c.Draw()\n",
    "c.SaveAs('dpt_res.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAI8CAIAAAC4XaJJAAAABmJLR0QAAAAAAAD5Q7t/AAAgAElE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\nAEAOyQ8AIIfkBwCQQ/IDAMgh+QEA5Px/MbFt7ihpQU0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = ROOT.TCanvas()\n",
    "\n",
    "hname = \"h2b\"\n",
    "h = h2b.Clone(\"h2b_clone\")\n",
    "\n",
    "if True:\n",
    "  gr1 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr2 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr3 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  for i in xrange(h.GetNbinsX()):\n",
    "    h_py = h.ProjectionY(\"_py\", i+1, i+1)\n",
    "    if h_py.Integral() < 15:  continue\n",
    "    #r = h_py.Fit(\"gaus\", \"SNQ\")\n",
    "    r = h_py.Fit(\"gaus\", \"SNQ\", \"\", h_py.GetMean() - 0.04*5, h_py.GetMean() + 0.04*5)\n",
    "    mean, sigma, meanErr, sigmaErr = r.Parameter(1), r.Parameter(2), r.ParError(1), r.ParError(2)\n",
    "    #mean, sigma, meanErr, sigmaErr = h_py.GetMean(), h_py.GetRMS(), 0, 0  #FIXME\n",
    "    gr1.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "    gr1.SetPointError(i, 0, 0, sigma, sigma)\n",
    "    gr2.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "    gr2.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "    #print h_py.Integral(), h_py.Integral(h_py.FindBin(mean-sigma), h_py.FindBin(mean+sigma))\n",
    "    loss = 1.0 - h_py.Integral(h_py.FindBin(mean-sigma), h_py.FindBin(mean+sigma)) / h_py.Integral()\n",
    "    gr3.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), loss)\n",
    "    gr3.SetPointError(i, 0, 0, 0, 0)\n",
    "  #\n",
    "  #gr1.Draw(\"ap\")\n",
    "  #gr2.Draw(\"ap\")\n",
    "  gr3.Draw(\"ap\")\n",
    "\n",
    "c.Draw()\n",
    "c.SaveAs('h2b_clone.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################## PLEASE IGNORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  print x_train.shape, y_train.shape, x_mask_train.shape\n",
    "\n",
    "  fig, axs = plt.subplots(80/4, 4, figsize=(4*4,4*80/4), tight_layout=True)\n",
    "\n",
    "  for i in xrange(x_train.shape[1]):\n",
    "    mask = x_mask_train[...,(i%nlayers)]\n",
    "    mask = mask.astype(np.bool)\n",
    "    if i >= (nlayers*5):\n",
    "      mask *= False\n",
    "\n",
    "    #fig, ax = plt.subplots(tight_layout=True)\n",
    "    xmin, xmax = -3, 3\n",
    "    ymin, ymax = -0.6, 0.6\n",
    "    #if (nlayers) <= i < (nlayers*2):\n",
    "    #  xmin, xmax = -1, 1\n",
    "    #elif i == 75:\n",
    "    #  xmin, xmax = -0.5, 1.5\n",
    "    #elif 76 <= i < 81:\n",
    "    #  xmin, xmax = -0.5, 1.5\n",
    "    \n",
    "    hist = axs[(i/4, i%4)].hist2d(x_train[...,i][~mask], y_train[~mask], bins=40, range=[[xmin, xmax], [ymin, ymax]], cmap=plt.cm.viridis)  #norm=colors.LogNorm(),\n",
    "    if x_train[...,i][~mask].size > 0:\n",
    "      print i, np.std(x_train[...,i][~mask]), np.percentile(x_train[...,i][~mask], [2,98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  df_x = pd.DataFrame(x_train)\n",
    "  df_y = pd.DataFrame(y_train)\n",
    "  df = pd.concat([df_x, df_y], axis = 1)\n",
    "  print df_x.shape, df_y.shape, df.shape\n",
    "  #print df\n",
    "  \n",
    "  df_corr = df.corr()\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.imshow(df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  x_copy  = the_variables.copy()\n",
    "  y_copy  = the_parameters.copy()\n",
    "  x_phi   = x_copy[:, nlayers*0:nlayers*1]\n",
    "  x_theta = x_copy[:, nlayers*1:nlayers*2]\n",
    "  x_bend  = x_copy[:, nlayers*2:nlayers*3]\n",
    "  x_mask  = x_copy[:, nlayers*3:nlayers*4]\n",
    "  x_road  = x_copy[:, nlayers*4:nlayers*5]\n",
    "\n",
    "  #x_phi_median    = x_road[:, 2] * 32 + 16  # multiply by 'quadstrip' unit (4 * 8)\n",
    "  x_phi_median    = x_road[:, 2] * 16 + 8  # multiply by 'doublestrip' unit (2 * 8)\n",
    "  x_phi_median    = x_phi_median[:, np.newaxis]\n",
    "  x_phi          -= x_phi_median\n",
    "\n",
    "  x_theta_median  = np.nanmedian(x_theta, axis=1)\n",
    "  x_theta_median  = x_theta_median[:, np.newaxis]\n",
    "  x_theta        -= x_theta_median\n",
    "\n",
    "  x_copy -= encoder.x_mean\n",
    "  x_copy /= encoder.x_std\n",
    "\n",
    "  df_x = pd.DataFrame(x_copy[:, :nlayers*3])\n",
    "  df_y = pd.DataFrame(y_copy[:, 0])\n",
    "  df = pd.concat([df_x, df_y], axis = 1)\n",
    "  print df_x.shape, df_y.shape, df.shape\n",
    "  #print df\n",
    "  \n",
    "  df_corr = df.corr()\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.imshow(df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  yy = df_corr.iloc[75]\n",
    "  yy = yy ** 2\n",
    "  #print yy\n",
    "\n",
    "  yyy = yy.sort_values(ascending=False)\n",
    "  pd.set_option('display.max_rows',1000)\n",
    "  print yyy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  fig, ax = plt.subplots(tight_layout=True)\n",
    "\n",
    "  i = 50  # ME1/1 bend\n",
    "  mask = x_mask_train[...,(i%25)]\n",
    "  mask = mask.astype(np.bool)\n",
    "\n",
    "  hist = ax.hist(x_train[...,i][~mask], bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  fig, ax = plt.subplots(tight_layout=True)\n",
    "  hist = ax.hist(x_train[...,75] - 1.0, bins=40)\n",
    "  \n",
    "  fig, ax = plt.subplots(tight_layout=True)\n",
    "  hist = ax.hist((encoder.x_theta_median - 3)/83, bins=40)\n",
    "  \n",
    "  print np.mean(x_train[...,75] - 1.0), np.std(x_train[...,75] - 1.0)\n",
    "  print np.mean((encoder.x_theta_median - 3)/83), np.std((encoder.x_theta_median - 3)/83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def l1_loss_f(x):\n",
    "  return np.abs(x)\n",
    "\n",
    "def l2_loss_f(x):\n",
    "  return np.square(x)\n",
    "\n",
    "def huber_loss_f(x, delta=1.345):\n",
    "  x = np.abs(x)\n",
    "  squared_loss = 0.5*np.square(x)\n",
    "  absolute_loss = delta * (x - 0.5*delta)\n",
    "  return np.where(x < delta, squared_loss, absolute_loss)\n",
    "\n",
    "def smooth_huber_loss_f(x):\n",
    "  x = np.abs(x)\n",
    "  return x + np.log(1 + np.exp(-2 * x))\n",
    "\n",
    "def pseudo_huber_loss_f(x, delta=2.):\n",
    "  delta2 = delta*delta\n",
    "  return delta2 * (np.sqrt(1 + np.square(x)/delta2) - 1)\n",
    "\n",
    "if False:\n",
    "  loss_x = np.linspace(-5., 5., 1000)\n",
    "  l1_loss_y = np.apply_along_axis(l1_loss_f, -1, loss_x)\n",
    "  l2_loss_y = np.apply_along_axis(l2_loss_f, -1, loss_x)\n",
    "  huber_loss_y = np.apply_along_axis(huber_loss_f, -1, loss_x)\n",
    "  smooth_huber_loss_y = np.apply_along_axis(smooth_huber_loss_f, -1, loss_x)\n",
    "  pseudo_huber_loss_y = np.apply_along_axis(pseudo_huber_loss_f, -1, loss_x)\n",
    "\n",
    "  plt.plot(loss_x, l2_loss_y, label='L2')\n",
    "  plt.plot(loss_x, l1_loss_y, label='L1')\n",
    "  plt.plot(loss_x, huber_loss_y, label='huber')\n",
    "  #plt.plot(loss_x, smooth_huber_loss_y, label='smooth huber')\n",
    "  plt.plot(loss_x, pseudo_huber_loss_y, label='pseudo huber')\n",
    "  plt.ylim(0,6)\n",
    "  plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  from keras.models import load_model\n",
    "  loaded_model = load_model('model.8.h5', custom_objects={'huber_loss': huber_loss})\n",
    "  loaded_model.load_weights('model_weights.8.h5')\n",
    "\n",
    "  maxEvents = 100\n",
    "  x_check = x[:maxEvents]\n",
    "  y_check = y[:maxEvents]\n",
    "\n",
    "  y_pred_check = loaded_model.predict(x_check)\n",
    "  y_pred_check = y_pred_check[:,0]\n",
    "\n",
    "  for i in xrange(maxEvents):\n",
    "    print i\n",
    "    print y_check[i], y_pred_check[i]\n",
    "    print x_check[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
