{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using numpy 1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using keras 2.0.5\n",
      "[INFO] Using tensorflow 1.1.0\n",
      "[INFO] Using sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "import random\n",
    "random.seed(2023)\n",
    "print('[INFO] Using numpy {0}'.format(np.__version__))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "old_stdout = sys.stdout\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, BatchNormalization\n",
    "from keras import initializers, regularizers, optimizers, losses\n",
    "#K.set_epsilon(1e-08)\n",
    "print('[INFO] Using keras {0}'.format(keras.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print('[INFO] Using tensorflow {0}'.format(tf.__version__))\n",
    "\n",
    "import sklearn\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('[INFO] Using sklearn {0}'.format(sklearn.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Globals\n",
    "nlayers = 12  # 5 (CSC) + 4 (RPC) + 3 (GEM)\n",
    "\n",
    "nvariables = 68\n",
    "\n",
    "do_skim = False\n",
    "\n",
    "add_noise = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(object):\n",
    "\n",
    "  def __init__(self, x, y, adjust_scale=0):\n",
    "    if x is not None and y is not None:\n",
    "      assert(x.shape[1] == (nlayers * 6) + 4)\n",
    "      assert(y.shape[1] == 3)\n",
    "      assert(x.shape[0] == y.shape[0])\n",
    "\n",
    "      self.nentries = x.shape[0]\n",
    "      self.x_orig  = x\n",
    "      self.y_orig  = y\n",
    "      self.x_copy  = x.copy()\n",
    "      self.y_copy  = y.copy()\n",
    "\n",
    "      # Get views\n",
    "      self.x_phi   = self.x_copy[:, nlayers*0:nlayers*1]\n",
    "      self.x_theta = self.x_copy[:, nlayers*1:nlayers*2]\n",
    "      self.x_bend  = self.x_copy[:, nlayers*2:nlayers*3]\n",
    "      self.x_ring  = self.x_copy[:, nlayers*3:nlayers*4]\n",
    "      self.x_fr    = self.x_copy[:, nlayers*4:nlayers*5]\n",
    "      self.x_mask  = self.x_copy[:, nlayers*5:nlayers*6].astype(np.bool)  # this makes a copy\n",
    "      self.x_road  = self.x_copy[:, nlayers*6:nlayers*7]  # ipt, ieta, iphi, iphi_corr\n",
    "      self.y_pt    = self.y_copy[:, 0]  # q/pT\n",
    "      self.y_phi   = self.y_copy[:, 1]\n",
    "      self.y_eta   = self.y_copy[:, 2]\n",
    "      \n",
    "      # Make event weight\n",
    "      #self.w       = np.ones(self.y_pt.shape, dtype=np.float32)\n",
    "      self.w       = np.abs(self.y_pt)/0.2 + 1.0\n",
    "      \n",
    "      # Straightness & zone\n",
    "      self.x_straightness = self.x_road[:, 0][:, np.newaxis]\n",
    "      self.x_zone         = self.x_road[:, 1][:, np.newaxis]\n",
    "      \n",
    "      # Subtract median phi from hit phis\n",
    "      #self.x_phi_median    = self.x_road[:, 2] * 32 - 16  # multiply by 'quadstrip' unit (4 * 8)\n",
    "      self.x_phi_median    = self.x_road[:, 2] * 16 - 8  # multiply by 'doublestrip' unit (2 * 8)\n",
    "      self.x_phi_median    = self.x_phi_median[:, np.newaxis]\n",
    "      self.x_phi          -= self.x_phi_median\n",
    "      \n",
    "      # Subtract median theta from hit thetas\n",
    "      self.x_theta_median  = np.nanmedian(self.x_theta[:,:5], axis=1)  # CSC only\n",
    "      self.x_theta_median[np.isnan(self.x_theta_median)] = np.nanmedian(self.x_theta[np.isnan(self.x_theta_median)], axis=1)  # use all\n",
    "      self.x_theta_median  = self.x_theta_median[:, np.newaxis]\n",
    "      self.x_theta        -= self.x_theta_median\n",
    "      \n",
    "      # Standard scales\n",
    "      if adjust_scale == 0:  # do not adjust\n",
    "        pass\n",
    "      elif adjust_scale == 1:  # use mean and std\n",
    "        self.x_mean  = np.nanmean(self.x_copy, axis=0)\n",
    "        self.x_std   = np.nanstd(self.x_copy, axis=0)\n",
    "        self.x_std   = self._handle_zero_in_scale(self.x_std)\n",
    "        self.x_copy -= self.x_mean\n",
    "        self.x_copy /= self.x_std\n",
    "      elif adjust_scale == 2:  # adjust by hand\n",
    "        self.x_phi   *= 0.000991  # GE1/1 dphi linear correlation with q/pT\n",
    "        self.x_theta *= (1/12.)   # 12 integer theta units\n",
    "        self.x_bend  *= 0.188082  # ME1/2 bend linear correlation with q/pT\n",
    "        x_ring_tmp    = self.x_ring.astype(np.int32)\n",
    "        x_ring_tmp    = (x_ring_tmp == 1) | (x_ring_tmp == 4)\n",
    "        self.x_ring[x_ring_tmp] = 0  # ring 1,4 -> 0\n",
    "        self.x_ring[~x_ring_tmp] = 1 # ring 2,3 -> 1\n",
    "        #self.x_fr     = self.x_fr\n",
    "      \n",
    "      # Remove outlier hits by checking hit thetas\n",
    "      if adjust_scale == 0:  # do not adjust\n",
    "        x_theta_tmp = np.abs(self.x_theta) > 10000.0\n",
    "      elif adjust_scale == 1:  # use mean and std\n",
    "        x_theta_tmp = np.abs(self.x_theta) > 1.0\n",
    "      elif adjust_scale == 2:  # adjust by hand\n",
    "        theta_cuts    = np.array((6., 6., 6., 6., 6., 12., 12., 12., 12., 9., 9., 9.), dtype=np.float32)\n",
    "        theta_cuts   *= (1/12.)   # 12 integer theta units\n",
    "        assert(len(theta_cuts) == nlayers)\n",
    "        x_theta_tmp = np.abs(self.x_theta) > theta_cuts\n",
    "      self.x_phi  [x_theta_tmp] = np.nan\n",
    "      self.x_theta[x_theta_tmp] = np.nan\n",
    "      self.x_bend [x_theta_tmp] = np.nan\n",
    "      self.x_ring [x_theta_tmp] = np.nan\n",
    "      self.x_fr   [x_theta_tmp] = np.nan\n",
    "      self.x_mask [x_theta_tmp] = 1.0\n",
    "      \n",
    "      # Add variables: straightness, zone, theta_median and mode variables\n",
    "      self.x_straightness -= 6.  # scaled to [-1,1]\n",
    "      self.x_straightness /= 6.\n",
    "      self.x_zone         -= 0.  # scaled to [0,1]\n",
    "      self.x_zone         /= 5.\n",
    "      self.x_theta_median -= 3.  # scaled to [0,1]\n",
    "      self.x_theta_median /= 83.\n",
    "      hits_to_station = np.array((5,1,2,3,4,1,2,3,4,5,2,5), dtype=np.int32)  # '5' denotes ME1/1\n",
    "      assert(len(hits_to_station) == nlayers)\n",
    "      self.x_mode_vars = np.zeros((self.nentries, 5), dtype=np.bool)\n",
    "      self.x_mode_vars[:,0] = np.any(self.x_mask[:,hits_to_station == 5] == 0, axis=1)\n",
    "      self.x_mode_vars[:,1] = np.any(self.x_mask[:,hits_to_station == 1] == 0, axis=1)\n",
    "      self.x_mode_vars[:,2] = np.any(self.x_mask[:,hits_to_station == 2] == 0, axis=1)\n",
    "      self.x_mode_vars[:,3] = np.any(self.x_mask[:,hits_to_station == 3] == 0, axis=1)\n",
    "      self.x_mode_vars[:,4] = np.any(self.x_mask[:,hits_to_station == 4] == 0, axis=1)\n",
    "      \n",
    "      # Remove NaN\n",
    "      #np.nan_to_num(self.x_copy, copy=False)\n",
    "      self.x_copy[np.isnan(self.x_copy)] = 0.0\n",
    "\n",
    "  # Copied from scikit-learn\n",
    "  def _handle_zero_in_scale(self, scale):\n",
    "    scale[scale == 0.0] = 1.0\n",
    "    return scale\n",
    "\n",
    "  def get_x(self):\n",
    "    #x_new = self.x_phi\n",
    "    x_new = np.hstack((self.x_phi, self.x_theta, self.x_bend, self.x_ring, self.x_fr, self.x_straightness, self.x_zone, self.x_theta_median, self.x_mode_vars))\n",
    "    return x_new\n",
    "\n",
    "  def get_x_mask(self):\n",
    "    x_mask = self.x_mask.copy()\n",
    "    return x_mask\n",
    "\n",
    "  def get_y(self):\n",
    "    y_new = self.y_pt.copy()\n",
    "    return y_new\n",
    "\n",
    "  def get_w(self):\n",
    "    w_new = self.w.copy()\n",
    "    return w_new\n",
    "\n",
    "  def save_encoder(self, filepath):\n",
    "    np.savez_compressed(filepath, x_mean=self.x_mean, x_std=self.x_std)\n",
    "\n",
    "  def load_endcoder(self, filepath):\n",
    "    loaded = np.load(filepath)\n",
    "    self.x_mean = loaded['x_mean']\n",
    "    self.x_std = loaded['x_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LeakyReLU with fix\n",
    "# https://github.com/keras-team/keras/pull/7784\n",
    "\n",
    "from keras.engine import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "class NewLeakyReLU(Layer):\n",
    "    \"\"\"Leaky version of a Rectified Linear Unit.\n",
    "    It allows a small gradient when the unit is not active:\n",
    "    `f(x) = alpha * x for x < 0`,\n",
    "    `f(x) = x for x >= 0`.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as the input.\n",
    "    # Arguments\n",
    "        alpha: float >= 0. Negative slope coefficient.\n",
    "    # References\n",
    "        - [Rectifier Nonlinearities Improve Neural Network Acoustic Models](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.3, **kwargs):\n",
    "        super(NewLeakyReLU, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        if isinstance(alpha, dict):\n",
    "            self.alpha = K.cast_to_floatx(alpha['value'])\n",
    "        else:\n",
    "            self.alpha = K.cast_to_floatx(alpha)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return K.relu(inputs, alpha=self.alpha)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'alpha': float(self.alpha)}\n",
    "        base_config = super(NewLeakyReLU, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Huber loss\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.345):\n",
    "  x = K.abs(y_true - y_pred)\n",
    "  squared_loss = 0.5*K.square(x)\n",
    "  absolute_loss = delta * (x - 0.5*delta)\n",
    "  #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "  xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "  return K.mean(xx, axis=-1)\n",
    "\n",
    "def masked_huber_loss(y_true, y_pred, delta=1.345):\n",
    "  x = K.abs(y_true - y_pred)\n",
    "  squared_loss = 0.5*K.square(x)\n",
    "  absolute_loss = delta * (x - 0.5*delta)\n",
    "  #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "  xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "\n",
    "  mask_value = 100.\n",
    "  mask = K.equal(y_true, mask_value)\n",
    "  mask = K.clip(1 - K.cast(mask, K.floatx()), K.epsilon(), 1 - K.epsilon())\n",
    "  return K.mean(xx * mask, axis=-1)\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Binary crossentropy\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "def masked_binary_crossentropy(y_true, y_pred, from_logits=False):\n",
    "  target, output = y_true, y_pred\n",
    "\n",
    "  # transform back to logits\n",
    "  if not from_logits:\n",
    "    output = K.clip(output, K.epsilon(), 1 - K.epsilon())\n",
    "    output = K.log(output / (1 - output))\n",
    "  \n",
    "  xx =  tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n",
    "  #xx =  tf.nn.weighted_cross_entropy_with_logits(targets=target, logits=output, pos_weight=0.5)  # pos_weight < 1 decreases the false positive count\n",
    "\n",
    "  mask_value = 100.\n",
    "  mask = K.equal(y_true, mask_value)\n",
    "  mask = K.clip(1 - K.cast(mask, K.floatx()), K.epsilon(), 1 - K.epsilon())\n",
    "  return K.mean(xx * mask, axis=-1)\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Learning rate decay by epoch number\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "  if (epoch % 10) == 0:\n",
    "    lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, lr*0.95)\n",
    "    print(\"lr changed to {}\".format(lr*0.95))\n",
    "  return K.get_value(model.optimizer.lr)\n",
    "\n",
    "lr_decay = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading muon data ...\n",
      "[INFO] Loaded the variables with shape (3646057, 76)\n",
      "[INFO] Loaded the parameters with shape (3646057, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-numpy/1.12.1-mlhled2/lib/python2.7/site-packages/numpy-1.12.1-py2.7-linux-x86_64.egg/numpy/lib/function_base.py:3858: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n",
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs/5.0-ghjeda6/lib/python2.7/site-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "#### Load data ####\n",
    "\n",
    "def muon_data():\n",
    "  try:\n",
    "    print('[INFO] Loading muon data ...')\n",
    "    infile = '../test2/histos_tba.12.npz'\n",
    "    loaded = np.load(infile)\n",
    "    the_variables = loaded['variables']\n",
    "    the_parameters = loaded['parameters']\n",
    "    print('[INFO] Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "    print('[INFO] Loaded the parameters with shape {0}'.format(the_parameters.shape))\n",
    "  except:\n",
    "    print('[ERROR] Failed to load data from file: {0}'.format(infile))\n",
    "\n",
    "  if do_skim:\n",
    "    the_variables = the_variables[:1000]\n",
    "    the_parameters = the_parameters[:1000]\n",
    "\n",
    "  encoder = Encoder(the_variables, the_parameters, adjust_scale=2)\n",
    "  x, y, w, x_mask = encoder.get_x(), encoder.get_y(), encoder.get_w(), encoder.get_x_mask()\n",
    "  assert np.isfinite(x).all()\n",
    "\n",
    "  # Split dataset in training and testing\n",
    "  x_train, x_test, y_train, y_test, w_train, w_test, x_mask_train, x_mask_test = train_test_split(x, y, w, x_mask, test_size=0.4)\n",
    "  return x_train, x_test, y_train, y_test, w_train, w_test, x_mask_train, x_mask_test\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "x_train, x_test, y_train, y_test, w_train, w_test, x_mask_train, x_mask_test = muon_data()\n",
    "\n",
    "# Add output nodes\n",
    "\n",
    "labels = np.where(np.abs(1.0/y_train) > 14., 1., 100.)  # mask_value is set to 100\n",
    "y_train = [y_train, labels.astype(np.float32)]\n",
    "\n",
    "labels = np.where(np.abs(1.0/y_test) > 14., 1., 100.)  # mask_value is set to 100\n",
    "y_test = [y_test, labels.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading pileup data ...\n",
      "[INFO] Loaded the variables with shape (170275, 76)\n",
      "[INFO] Loaded the auxiliary info with shape (170275, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs/5.0-ghjeda6/lib/python2.7/site-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "#### Load data (pileup) ####\n",
    "\n",
    "def pileup_data():\n",
    "  try:\n",
    "    print('[INFO] Loading pileup data ...')\n",
    "    infile = '../test2/histos_tbd.test.npz'\n",
    "    loaded = np.load(infile)\n",
    "    the_variables = loaded['variables']\n",
    "    the_parameters = np.zeros((the_variables.shape[0], 3), dtype=np.float32)\n",
    "    the_auxiliaries = loaded['aux']\n",
    "    print('[INFO] Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "    print('[INFO] Loaded the auxiliary info with shape {0}'.format(the_auxiliaries.shape))\n",
    "  except:\n",
    "    print('[ERROR] Failed to load data from file: {0}'.format(infile))\n",
    "    \n",
    "  discr_pt_cut = the_auxiliaries[:,2] > 14.\n",
    "  the_variables = the_variables[~discr_pt_cut]\n",
    "  the_parameters = the_parameters[~discr_pt_cut]\n",
    "  the_auxiliaries = the_auxiliaries[~discr_pt_cut]\n",
    "\n",
    "  if do_skim:\n",
    "    the_variables = the_variables[:1000]\n",
    "    the_parameters = the_parameters[:1000]\n",
    "    the_auxiliaries = the_auxiliaries[:1000]\n",
    "\n",
    "  encoder = Encoder(the_variables, the_parameters, adjust_scale=2)\n",
    "  x, y, w, x_mask = encoder.get_x(), encoder.get_y(), encoder.get_w(), encoder.get_x_mask()\n",
    "  aux = the_auxiliaries  # jobid, ievt, highest_part_pt, highest_track_pt\n",
    "  assert np.isfinite(x).all()\n",
    "  \n",
    "  split = the_auxiliaries[:,0] < 50.\n",
    "  x_train, x_test, aux_train, aux_test = x[split], x[~split], aux[split], aux[~split]\n",
    "  return x_train, x_test, aux_train, aux_test\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "x_adv_train, x_adv_test, aux_adv_train, aux_adv_test = pileup_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Create a model ####\n",
    "\n",
    "# See https://keras.io/models/about-keras-models/\n",
    "#     https://keras.io/layers/about-keras-layers/\n",
    "#     https://keras.io/getting-started/functional-api-guide/#getting-started-with-the-keras-functional-api\n",
    "\n",
    "def create_model():\n",
    "  inputs = Input(shape=(nvariables,), dtype='float32')\n",
    "\n",
    "  x = Dense(64, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000))(inputs)\n",
    "  #x = Dropout(0.2)(x)\n",
    "  x = Dense(32, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000))(x)\n",
    "  #x = Dropout(0.2)(x)\n",
    "  x = Dense(16, activation='relu', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000))(x)\n",
    "  #x = Dropout(0.2)(x)\n",
    "  \n",
    "  regr = Dense(1, activation='linear', kernel_initializer='glorot_uniform', name='regr')(x)\n",
    "  discr = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform', name='discr')(x)\n",
    "\n",
    "  # This creates a model that includes\n",
    "  # the Input layer, three Dense layers and the Output layer\n",
    "  model = Model(inputs=inputs, outputs=[regr, discr])\n",
    "\n",
    "  # Set loss and optimizers\n",
    "  #binary_crossentropy = losses.binary_crossentropy\n",
    "  #mean_squared_error = losses.mean_squared_error\n",
    "  \n",
    "  #adam = optimizers.Adam(lr=0.0001)\n",
    "  adam = optimizers.Adam(lr=0.001)\n",
    "  #adam = optimizers.Adam(lr=0.001, amsgrad=True)\n",
    "\n",
    "  # Compile\n",
    "  model.compile(optimizer=adam,\n",
    "    loss={'regr': masked_huber_loss, 'discr': masked_binary_crossentropy},\n",
    "    loss_weights={'regr': 10.0, 'discr': 1.0},\n",
    "    #metrics={'regr': ['acc', 'mse', 'mae'], 'discr': ['acc',]}\n",
    "    )\n",
    "  return model\n",
    "\n",
    "def save_model(model):\n",
    "  # Store model to file\n",
    "  model.summary()\n",
    "  model.save('model.h5')\n",
    "  model.save_weights('model_weights.h5')\n",
    "\n",
    "  # Store model to json\n",
    "  import json\n",
    "  with open('model.json', 'w') as outfile:\n",
    "    json.dump(model.to_json(), outfile)\n",
    "  return\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Training Functions ####\n",
    "\n",
    "# from https://github.com/keras-team/keras/blob/master/keras/utils/generic_utils.py\n",
    "def slice_arrays(arrays, start=None, stop=None):\n",
    "    \"\"\"Slices an array or list of arrays.\n",
    "    This takes an array-like, or a list of\n",
    "    array-likes, and outputs:\n",
    "        - arrays[start:stop] if `arrays` is an array-like\n",
    "        - [x[start:stop] for x in arrays] if `arrays` is a list\n",
    "    Can also work on list/array of indices: `_slice_arrays(x, indices)`\n",
    "    # Arguments\n",
    "        arrays: Single array or list of arrays.\n",
    "        start: can be an integer index (start index)\n",
    "            or a list/array of indices\n",
    "        stop: integer (stop index); should be None if\n",
    "            `start` was a list.\n",
    "    # Returns\n",
    "        A slice of the array(s).\n",
    "    \"\"\"\n",
    "    if arrays is None:\n",
    "        return [None]\n",
    "    elif isinstance(arrays, list):\n",
    "        if hasattr(start, '__len__'):\n",
    "            # hdf5 datasets only support list objects as indices\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return [None if x is None else x[start] for x in arrays]\n",
    "        else:\n",
    "            return [None if x is None else x[start:stop] for x in arrays]\n",
    "    else:\n",
    "        if hasattr(start, '__len__'):\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return arrays[start]\n",
    "        elif hasattr(start, '__getitem__'):\n",
    "            return arrays[start:stop]\n",
    "        else:\n",
    "            return [None]\n",
    "\n",
    "\n",
    "def merge_arrays(arrays, arrays_to_add):\n",
    "    if isinstance(arrays, list):\n",
    "        return [None if x is None else np.concatenate((x,y)) for (x,y) in zip(arrays, arrays_to_add)]\n",
    "    else:\n",
    "        return [None]\n",
    "\n",
    "# from https://github.com/keras-team/keras/blob/master/keras/engine/training_utils.py\n",
    "def make_batches(size, batch_size):\n",
    "    \"\"\"Returns a list of batch indices (tuples of indices).\n",
    "    # Arguments\n",
    "        size: Integer, total size of the data to slice into batches.\n",
    "        batch_size: Integer, batch size.\n",
    "    # Returns\n",
    "        A list of tuples of array indices.\n",
    "    \"\"\"\n",
    "    num_batches = (size + batch_size - 1) // batch_size  # round up\n",
    "    return [(i * batch_size, min(size, (i + 1) * batch_size))\n",
    "            for i in range(num_batches)]\n",
    "\n",
    "# from https://github.com/keras-team/keras/blob/2.0.5/keras/engine/training.py\n",
    "import warnings\n",
    "import copy\n",
    "from keras import callbacks as cbks\n",
    "from keras.utils.generic_utils import Progbar\n",
    "\n",
    "def train(model, x, y, x_adv, aux_adv, batch_size=None, epochs=1, verbose=1, callbacks=None,\n",
    "          validation_split=0., shuffle=True, class_weight=None, sample_weight=None):\n",
    "\n",
    "  # Validate user data.\n",
    "  x, y, sample_weights = model._standardize_user_data(\n",
    "    x, y,\n",
    "    sample_weight=sample_weight,\n",
    "    class_weight=class_weight,\n",
    "    batch_size=batch_size)\n",
    "  ins = x + y + sample_weights\n",
    "  \n",
    "  # Prepare validation data.\n",
    "  do_validation = False\n",
    "  if validation_split and 0. < validation_split < 1.:\n",
    "    do_validation = True\n",
    "    if hasattr(x[0], 'shape'):\n",
    "      split_at = int(x[0].shape[0] * (1. - validation_split))\n",
    "    else:\n",
    "      split_at = int(len(x[0]) * (1. - validation_split))\n",
    "    x, val_x = (slice_arrays(x, 0, split_at), slice_arrays(x, split_at))\n",
    "    y, val_y = (slice_arrays(y, 0, split_at), slice_arrays(y, split_at))\n",
    "    sample_weights, val_sample_weights = (slice_arrays(sample_weights, 0, split_at), slice_arrays(sample_weights, split_at))\n",
    "    val_ins = val_x + val_y + val_sample_weights\n",
    "  else:\n",
    "    val_ins = []\n",
    "\n",
    "  # logic from `_fit_loop()`\n",
    "  num_train_samples = x[0].shape[0]\n",
    "  index_array = np.arange(num_train_samples)\n",
    "  num_test_samples = val_x[0].shape[0]\n",
    "  val_index_array = np.arange(num_test_samples)\n",
    "  \n",
    "  # Callbacks\n",
    "  out_labels = model.metrics_names\n",
    "  if do_validation:\n",
    "    callback_metrics = copy.copy(out_labels) + ['val_' + n for n in out_labels]\n",
    "  else:\n",
    "    callback_metrics = copy.copy(out_labels)\n",
    "\n",
    "  model.history = cbks.History()\n",
    "  callbacks = [cbks.BaseLogger()] + (callbacks or []) + [model.history]\n",
    "  if verbose:\n",
    "    callbacks += [cbks.ProgbarLogger()]\n",
    "  callbacks = cbks.CallbackList(callbacks)\n",
    "  callback_model = model\n",
    "  callbacks.set_model(callback_model)\n",
    "  callbacks.set_params({\n",
    "      'batch_size': batch_size,\n",
    "      'epochs': epochs,\n",
    "      'samples': num_train_samples,\n",
    "      'verbose': verbose,\n",
    "      'do_validation': do_validation,\n",
    "      'metrics': callback_metrics or [],\n",
    "  })\n",
    "  callbacks.on_train_begin()\n",
    "  callback_model.stop_training = False\n",
    "  for cbk in callbacks:\n",
    "      cbk.validation_data = val_ins\n",
    "  \n",
    "  \n",
    "  # Loop over epochs\n",
    "  for epoch in xrange(epochs):\n",
    "    epoch_logs = {}\n",
    "    callbacks.on_epoch_begin(epoch)\n",
    "    \n",
    "    if shuffle:\n",
    "      np.random.shuffle(index_array)\n",
    "      #np.random.shuffle(val_index_array)\n",
    "    \n",
    "    batches = make_batches(num_train_samples, batch_size)\n",
    "    \n",
    "    # Loop over batches\n",
    "    for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "      batch_ids = index_array[batch_start:batch_end]\n",
    "      ins_batch = slice_arrays(ins, batch_ids)\n",
    "      assert isinstance(ins_batch, list) and len(ins_batch) == 1 + 2 + 2\n",
    "      \n",
    "      # Add noise (pileup)\n",
    "      if add_noise:\n",
    "        noise = x_adv[np.random.randint(0, x_adv.shape[0], ins_batch[0].shape[0])]\n",
    "        noise_reg = np.zeros_like(ins_batch[1]) + 100.  # mask_value is set to 100\n",
    "        noise_discr = np.zeros_like(ins_batch[2])\n",
    "        noise_reg_w = np.ones_like(ins_batch[3])\n",
    "        noise_discr_w = np.ones_like(ins_batch[3])\n",
    "        ins_noise = [noise, noise_reg, noise_discr, noise_reg_w, noise_discr_w]\n",
    "        ins_batch = merge_arrays(ins_batch, ins_noise)\n",
    "      \n",
    "      batch_logs = {}\n",
    "      batch_logs['batch'] = batch_index\n",
    "      batch_logs['size'] = len(batch_ids)\n",
    "      callbacks.on_batch_begin(batch_index, batch_logs)\n",
    "      \n",
    "      # Magic\n",
    "      model._make_train_function()\n",
    "      f = model.train_function\n",
    "      outs = f(ins_batch)\n",
    "      \n",
    "      if not isinstance(outs, list):\n",
    "        outs = [outs]\n",
    "      for l, o in zip(out_labels, outs):\n",
    "        batch_logs[l] = o\n",
    "      \n",
    "      callbacks.on_batch_end(batch_index, batch_logs)\n",
    "      if callback_model.stop_training:\n",
    "        break\n",
    "      \n",
    "      if batch_index == len(batches) - 1:  # Last batch.\n",
    "        if do_validation:\n",
    "          # logic from `_test_loop()`\n",
    "          val_batches = make_batches(num_test_samples, batch_size)\n",
    "          val_outs = []\n",
    "          if verbose == 1:\n",
    "            progbar = Progbar(target=num_test_samples)\n",
    "          \n",
    "          for val_batch_index, (val_batch_start, val_batch_end) in enumerate(val_batches):\n",
    "            val_batch_ids = val_index_array[val_batch_start:val_batch_end]\n",
    "            val_ins_batch = slice_arrays(val_ins, val_batch_ids)\n",
    "            \n",
    "            # Magic\n",
    "            model._make_test_function()\n",
    "            val_f = model.test_function\n",
    "            val_batch_outs = val_f(val_ins_batch)\n",
    "            \n",
    "            if isinstance(val_batch_outs, list):\n",
    "              if val_batch_index == 0:\n",
    "                for i, val_batch_out in enumerate(val_batch_outs):\n",
    "                  val_outs.append(0.)\n",
    "              for i, val_batch_out in enumerate(val_batch_outs):\n",
    "                val_outs[i] += val_batch_out * len(val_batch_ids)\n",
    "            else:\n",
    "              if val_batch_index == 0:\n",
    "                val_outs.append(0.)\n",
    "              val_outs[0] += val_batch_outs * len(val_batch_ids)\n",
    "            \n",
    "            if verbose == 1:\n",
    "              progbar.update(val_batch_end)\n",
    "            \n",
    "          if not isinstance(val_outs, list):\n",
    "            val_outs = [val_outs]\n",
    "          # Same labels assumed.\n",
    "          for l, o in zip(out_labels, val_outs):\n",
    "            o /= num_test_samples\n",
    "            o /= 2.  #FIXME\n",
    "            epoch_logs['val_' + l] = o\n",
    "            \n",
    "    callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "    if callback_model.stop_training:\n",
    "      break\n",
    "\n",
    "  callbacks.on_train_end()\n",
    "  return model.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Begin training ...\n"
     ]
    }
   ],
   "source": [
    "#### Training ####\n",
    "\n",
    "print('[INFO] Begin training ...')\n",
    "assert keras.backend.backend() == 'tensorflow'\n",
    "\n",
    "start_time = time.time()\n",
    "sys.stdout = open('keras_output_1.txt', 'w')\n",
    "#history = model.fit(x_train, y_train, epochs=40, validation_split=0.1, batch_size=256, verbose=1)\n",
    "#history = model.fit(x_train, y_train, epochs=400, validation_split=0.1, batch_size=256, callbacks=[lr_decay], verbose=0)\n",
    "history = train(model, x_train, y_train, x_adv_train, aux_adv_train, epochs=10, validation_split=0.1, batch_size=256, verbose=1)\n",
    "#history = train(model, x_train, y_train, x_adv_train, aux_adv_train, epochs=400, validation_split=0.1, batch_size=256, callbacks=[lr_decay], verbose=0)\n",
    "sys.stdout.close()\n",
    "sys.stdout = old_stdout\n",
    "print('[INFO] Time elapsed: {0} sec'.format(time.time() - start_time))\n",
    "\n",
    "save_model(model)\n",
    "print('[INFO] Done training.')\n",
    "print('[INFO] Model is saved as model.h5, model.json and model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Evaluation ####\n",
    "\n",
    "#loss_and_metrics = model.evaluate(x_test, y_test, sample_weight=w_test, batch_size=1000)\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=1000)\n",
    "print('[INFO] loss and metrics: {0}'.format(loss_and_metrics))\n",
    "\n",
    "# Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['regr_loss'])\n",
    "plt.plot(history.history['val_regr_loss'])\n",
    "plt.title('regr loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['discr_loss'])\n",
    "plt.plot(history.history['val_discr_loss'])\n",
    "plt.title('discr loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "## Mean Squared Error\n",
    "#plt.plot(history.history['regr_mean_squared_error'])\n",
    "#plt.plot(history.history['val_regr_mean_squared_error'])\n",
    "#plt.title('regr mse')\n",
    "#plt.ylabel('mse')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "## Mean Absolute Error\n",
    "#plt.plot(history.history['regr_mean_absolute_error'])\n",
    "#plt.plot(history.history['val_regr_mean_absolute_error'])\n",
    "#plt.title('regr mae')\n",
    "#plt.ylabel('mae')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "## Accuracy\n",
    "#plt.plot(history.history['discr_acc'])\n",
    "#plt.plot(history.history['val_discr_acc'])\n",
    "#plt.title('discr accuracy')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Make plots ####\n",
    "\n",
    "from keras.models import load_model\n",
    "import ROOT\n",
    "\n",
    "# Load model\n",
    "model_file = 'model.h5'\n",
    "model_weights_file = 'model_weights.h5'\n",
    "\n",
    "custom_objects = {'masked_huber_loss': masked_huber_loss, 'masked_binary_crossentropy': masked_binary_crossentropy}\n",
    "loaded_model = load_model(model_file, custom_objects=custom_objects)\n",
    "loaded_model.load_weights(model_weights_file)\n",
    "\n",
    "# Set styles\n",
    "ROOT.gROOT.LoadMacro(\"tdrstyle.C\")\n",
    "ROOT.gROOT.ProcessLine(\"setTDRStyle();\")\n",
    "ROOT.gStyle.SetPalette(57)  # kBird\n",
    "ROOT.gStyle.SetMarkerStyle(1)\n",
    "ROOT.gStyle.SetEndErrorSize(0)\n",
    "ROOT.gStyle.SetPadGridX(True)\n",
    "ROOT.gStyle.SetPadGridY(True)\n",
    "\n",
    "nentries_test = x_test.shape[0]/10\n",
    "#nentries_test = 100000\n",
    "\n",
    "y_test_meas = loaded_model.predict(x_test[:nentries_test, :])\n",
    "\n",
    "y_adv_test = [np.zeros((x_adv_test.shape[0],1), dtype=np.float32), np.zeros((x_adv_test.shape[0],1), dtype=np.float32)]\n",
    "y_adv_test_meas = loaded_model.predict(x_adv_test)\n",
    "\n",
    "print y_test_meas\n",
    "print y_adv_test_meas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For regression\n",
    "h1 = ROOT.TH1F(\"h1\", \"h1\", 300, -0.3, 0.3)\n",
    "h2a = ROOT.TH2F(\"h2a\", \"h2a\", 100, -0.5, 0.5, 300, -0.3, 0.3)\n",
    "h2b = ROOT.TH2F(\"h2b\", \"h2b\", 100, -0.5, 0.5, 300, -0.5, 0.5)\n",
    "h2c = ROOT.TH2F(\"h2c\", \"h2c\", 100, -0.5, 0.5, 400, -2, 2)\n",
    "h2d = ROOT.TH2F(\"h2d\", \"h2d\", 100, -0.5, 0.5, 400, -2, 2)\n",
    "\n",
    "for i in xrange(nentries_test):\n",
    "  y_true = y_test[0][i]\n",
    "  y_meas = y_test_meas[0][i]\n",
    "  h1.Fill(y_meas - y_true)\n",
    "  h2a.Fill(y_true, y_meas - y_true) \n",
    "  h2b.Fill(y_true, y_meas)\n",
    "  h2c.Fill(y_true, (y_meas - y_true)/abs(y_true))\n",
    "  h2d.Fill(abs(y_true), (abs(y_meas) - abs(y_true))/abs(y_true)) \n",
    "\n",
    "\n",
    "c = ROOT.TCanvas()\n",
    "h1.SetMarkerStyle(20)\n",
    "h1.Draw()\n",
    "c.Draw()\n",
    "print h1.GetEntries(), h1.GetMean(), h1.GetRMS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2a.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2b.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2c.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2d.SetStats(0)\n",
    "h2d.SetTitle(\"\")\n",
    "h2d.GetXaxis().SetTitle(\"gen 1/p_{T} [1/GeV]\")\n",
    "h2d.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T}\")\n",
    "h2d.GetXaxis().SetRangeUser(0, 0.5)\n",
    "h2d.GetYaxis().SetRangeUser(-1, 2)\n",
    "h2d.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "\n",
    "hname = \"h2d\"\n",
    "h = h2d.Clone(\"h2d_clone\")\n",
    "#h.Draw(\"COLZ\")\n",
    "#gPad.Print(hname+\".png\")\n",
    "#h.RebinX(2)\n",
    "\n",
    "#h_pfx = h.ProfileX(hname+\"_pfx\", 1, -1, \"s\")\n",
    "#h_pfx.SetMaximum(1.2)\n",
    "#h_pfx.SetMinimum(-0.2)\n",
    "#h_pfx.Draw()\n",
    "#h_pfx.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "#gPad.Print(h_pfx.GetName()+\".png\")\n",
    "#\n",
    "\n",
    "if True:\n",
    "  # Apply gaussian fits\n",
    "  gr1 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr2 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr1_aspt = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr2_aspt = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  for i in xrange(h.GetNbinsX()):\n",
    "    h_py = h.ProjectionY(\"_py\", i+1, i+1)\n",
    "    if h_py.Integral() < 15:  continue\n",
    "    #r = h_py.Fit(\"gaus\", \"SNQ\")\n",
    "    r = h_py.Fit(\"gaus\", \"SNQ\", \"\", h_py.GetMean() - 0.04*8, h_py.GetMean() + 0.04*8)\n",
    "    mean, sigma, meanErr, sigmaErr = r.Parameter(1), r.Parameter(2), r.ParError(1), r.ParError(2)\n",
    "    gr1.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "    gr1.SetPointError(i, 0, 0, sigma, sigma)\n",
    "    gr2.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "    gr2.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "    gr1_aspt.SetPoint(i, 1.0/h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "    gr1_aspt.SetPointError(i, 0, 0, sigma, sigma)\n",
    "    gr2_aspt.SetPoint(i, 1.0/h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "    gr2_aspt.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "  #\n",
    "  hname1 = hname\n",
    "  h_pfx = h.ProfileX(hname1+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  h_pfx.Draw()\n",
    "  gr1.SetMarkerStyle(20)\n",
    "  gr1.Draw(\"p\")\n",
    "  #gr1.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #\n",
    "  hname2 = hname\n",
    "  h_pfx = h.ProfileX(hname2+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetMaximum(1)\n",
    "  h_pfx.SetMinimum(0)\n",
    "  h_pfx.Draw()\n",
    "  gr2.SetMarkerStyle(20)\n",
    "  gr2.Draw(\"p\")\n",
    "  #gr2.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #\n",
    "  hname1 = hname\n",
    "  h_pfx = h.ProfileX(hname1+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetBins(50, 0, 50)\n",
    "  h_pfx.GetXaxis().SetTitle(\"gen p_{T} [GeV]\")\n",
    "  h_pfx.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T} bias\")\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  h_pfx.Draw()\n",
    "  gr1_aspt.SetMarkerStyle(20)\n",
    "  gr1_aspt.Draw(\"p\")\n",
    "  #gr1_aspt.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #ROOT.gPad.SetLogx(1)\n",
    "  #ROOT.gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #ROOT.gPad.SetLogx(0)\n",
    "  #\n",
    "  hname2 = hname\n",
    "  h_pfx = h.ProfileX(hname2+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetStats(0)\n",
    "  h_pfx.SetBins(50, 0, 50)\n",
    "  h_pfx.GetXaxis().SetTitle(\"gen p_{T} [GeV]\")\n",
    "  h_pfx.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T} resolution\")\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  #h_pfx.SetMaximum(0.1)\n",
    "  #h_pfx.SetMinimum(-0.01)\n",
    "  h_pfx.Draw()\n",
    "  gr2_aspt.SetMarkerStyle(20)\n",
    "  gr2_aspt.Draw(\"p\")\n",
    "  #gr2_aspt.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  ROOT.gPad.SetLogx(1)\n",
    "  #ROOT.gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #ROOT.gPad.SetLogx(0)\n",
    "    \n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For classification\n",
    "hh1a = ROOT.TH1F(\"hh1a\", \"hh1a\", 120, -0.1, 1.1)\n",
    "hh1b = ROOT.TH1F(\"hh1b\", \"hh1b\", 120, -0.1, 1.1)\n",
    "\n",
    "for i in xrange(nentries_test):\n",
    "  if y_test[1][i] == 1.0:\n",
    "    hh1a.Fill(y_test_meas[1][i])\n",
    "\n",
    "for i in xrange(y_adv_test[1].shape[0]):\n",
    "  hh1b.Fill(y_adv_test_meas[1][i])\n",
    "\n",
    "\n",
    "c = ROOT.TCanvas()\n",
    "hh1a.SetLineColor(632)  # kRed\n",
    "hh1a.Scale(1.0/hh1a.Integral())\n",
    "hh1a.Draw(\"hist\")\n",
    "hh1b.SetLineColor(1)  # kBlack\n",
    "hh1b.Scale(1.0/hh1b.Integral())\n",
    "hh1b.Draw(\"same hist\")\n",
    "c.Draw()\n",
    "c.SetLogy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_true = np.concatenate((y_test[1][:nentries_test], y_adv_test[1]))\n",
    "y_pred = np.concatenate((y_test_meas[1], y_adv_test_meas[1]))\n",
    "\n",
    "mask = (y_true == 100.)  # mask_value is set to 100\n",
    "y_true = y_true[~mask]\n",
    "y_pred = y_pred[~mask]\n",
    "\n",
    "fpr, tpr, thresh = roc_curve(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(tight_layout=True)\n",
    "ax.plot(fpr, tpr)\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.set_title('ROC curve')\n",
    "ax.set_xlim([0.0,1.0])\n",
    "ax.set_ylim([0.9,1.0])\n",
    "\n",
    "idx = np.searchsorted(tpr, [0.9, 0.925, 0.95, 0.97, 0.98, 0.985, 0.99, 0.995])\n",
    "print tpr[idx]\n",
    "print fpr[idx]\n",
    "print thresh[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Build trigger ####\n",
    "\n",
    "class MyTriggerV1(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_trigger_pt(self, y_meas):\n",
    "    pt = np.abs(1.0/y_meas)\n",
    "    pt_clipped = np.clip(pt, 3., 60.)\n",
    "    pt = pt * (1.0 + (0.08813 + 0.009504 * pt_clipped) * 1.28155)  # erf(1.28155/sqrt(2)) = 0.8 [90% upper limit from -1 to -1]\n",
    "    return pt\n",
    "  \n",
    "  def pass_trigger(self, x, ndof, y_meas, y_discr):\n",
    "    trk_mode = 0\n",
    "    x_mode_vars = x[nlayers*5+3:nlayers*5+8].astype(np.bool)  # convert to booleans\n",
    "    for i, x_mode_var in enumerate(x_mode_vars):\n",
    "      if i == 0:\n",
    "        station = 1\n",
    "      else:\n",
    "        station = i\n",
    "      if x_mode_var:\n",
    "        trk_mode |= (1 << (4 - station))\n",
    "\n",
    "    pt_bins = (-0.50, -0.333333, -0.25, -0.20, -0.15, -0.10, -0.05, 0.05, 0.10, 0.15, 0.20, 0.25, 0.333333, 0.50)\n",
    "    def find_pt_bin(pt):\n",
    "      ipt = np.digitize((pt,), pt_bins[1:])[0]  # skip lowest edge\n",
    "      ipt = np.clip(ipt, 0, len(pt_bins)-2)\n",
    "      return ipt\n",
    "    def find_road_quality(ipt):\n",
    "      best_ipt = find_pt_bin(0.)\n",
    "      return best_ipt - abs(ipt - best_ipt)\n",
    "\n",
    "    ipt1 = (x[nlayers*5+0:nlayers*5+1] * 6 + 6).astype(np.int32)\n",
    "    ipt2 = find_pt_bin(y_meas)\n",
    "    quality1 = find_road_quality(ipt1)\n",
    "    quality2 = find_road_quality(ipt2)\n",
    "    \n",
    "    if trk_mode in (11,13,14,15) and quality2 <= (quality1+1):\n",
    "      if np.abs(1.0/y_meas) > 14.:\n",
    "        if ndof <= 3:\n",
    "          #trigger = (y_discr > 0.5)\n",
    "          trigger = (y_discr > 0.8)\n",
    "          #trigger = (y_discr > 0.95)\n",
    "        else:\n",
    "          trigger = (y_discr > 0.5393)\n",
    "          #trigger = (y_discr > 0.95)\n",
    "      else:\n",
    "        trigger = True\n",
    "    else:\n",
    "      trigger = False\n",
    "    return trigger\n",
    "\n",
    "\n",
    "class MyTriggerV2(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_trigger_pt(self, y_meas):\n",
    "    pt = np.abs(1.0/y_meas)\n",
    "    pt_clipped = np.clip(pt, 3., 60.)\n",
    "    pt = pt * (1.0 + (0.08813 + 0.009504 * pt_clipped) * 1.28155)  # erf(1.28155/sqrt(2)) = 0.8 [90% upper limit from -1 to -1]\n",
    "    return pt\n",
    "  \n",
    "  def pass_trigger(self, x, ndof, y_meas, y_discr):\n",
    "    trk_mode = 0\n",
    "    x_mode_vars = x[nlayers*5+3:nlayers*5+8].astype(np.bool)  # convert to booleans\n",
    "    for i, x_mode_var in enumerate(x_mode_vars):\n",
    "      if i == 0:\n",
    "        station = 1\n",
    "      else:\n",
    "        station = i\n",
    "      if x_mode_var:\n",
    "        trk_mode |= (1 << (4 - station))\n",
    "    \n",
    "    if trk_mode in (11,13,14,15):\n",
    "      if np.abs(1.0/y_meas) > 14.:\n",
    "        if ndof <= 3:\n",
    "          #trigger = (y_discr > 0.5)\n",
    "          trigger = (y_discr > 0.8)\n",
    "          #trigger = (y_discr > 0.95)\n",
    "        else:\n",
    "          trigger = (y_discr > 0.5393)\n",
    "          #trigger = (y_discr > 0.95)\n",
    "      else:\n",
    "        trigger = True\n",
    "    else:\n",
    "      trigger = False\n",
    "    return trigger\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "mytrigger = MyTriggerV2()\n",
    "\n",
    "from rootpy.plotting import Hist, Efficiency\n",
    "from math import sqrt\n",
    "histograms = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Make trigger efficiency ####\n",
    "\n",
    "eff_pt_bins = (0., 0.5, 1., 2., 3., 4., 5., 6., 8., 10., 12., 14., 16., 18., 20., 22., 24., 26., 28., 30., 35., 40., 45., 50., 60., 80., 120.)\n",
    "\n",
    "hname = \"x_eff_vs_genpt_l1pt20_denom\"\n",
    "h1c_denom = Hist(eff_pt_bins, name=hname, title=\"; gen p_{T} [GeV]\", type='F')\n",
    "hname = \"x_eff_vs_genpt_l1pt20_numer\"\n",
    "h1c_numer = Hist(eff_pt_bins, name=hname, title=\"; gen p_{T} [GeV]\", type='F')\n",
    "\n",
    "# Loop over events\n",
    "for x, ndof, y_meas, y_discr, y_true in zip(x_test, x_test, y_test_meas[0], y_test_meas[1], y_test[0]):\n",
    "\n",
    "  ndof = 4  #FIXME\n",
    "  \n",
    "  pt = mytrigger.get_trigger_pt(y_meas)\n",
    "  \n",
    "  trigger = mytrigger.pass_trigger(x, ndof, y_meas, y_discr)\n",
    "  trigger = trigger and (pt > 20.)\n",
    "  \n",
    "  pt_true = np.abs(1.0/y_true)\n",
    "  \n",
    "  h1c_denom.fill(pt_true)\n",
    "  if trigger:\n",
    "    h1c_numer.fill(pt_true)\n",
    "\n",
    "h1c_eff = Efficiency(h1c_numer, h1c_denom)\n",
    "h1c_eff.SetStatisticOption(0)  # kFCP\n",
    "h1c_eff.SetConfidenceLevel(0.682689492137)  # one sigma\n",
    "h1c_eff.SetMarkerStyle(1)\n",
    "h1c_eff.SetMarkerColor(800)  # kOrange\n",
    "h1c_eff.SetLineColor(800)  # kOrange\n",
    "h1c_eff.SetLineWidth(2)\n",
    "h1c_eff.SetDirectory(0)\n",
    "histograms['h1c_numer'] = h1c_numer\n",
    "histograms['h1c_denom'] = h1c_denom\n",
    "histograms['h1c_eff'] = h1c_eff\n",
    "\n",
    "# Add corrections\n",
    "if False:\n",
    "  nbinsx = h1c_eff.GetTotalHistogram().GetNbinsX()\n",
    "  corrections = [0.0, 0.0, 0.0, 2.5596844660416443e-05, 9.071900979369844e-05, -0.00025407866698875234, 3.9318943372533294e-05, 0.00046750609322471475, -0.0012868531325132167, -0.0017629378199139414, -0.003412967281340079, -1.859263302672609e-05, -0.0027697063960877566, -0.011912089953303617, -0.012565904385017701, -0.01502952024543791, -0.007091832957321742, -0.011625792518549893, -0.017156862745097978, -0.007722007722007707, -0.008966446308134701, -0.00857140366324638, -0.021269462895507463, -0.009598486441034226, -0.01485925792486198, -0.010991760558270336]\n",
    "  assert(len(corrections) == nbinsx)\n",
    "  for b in xrange(1,nbinsx+1):\n",
    "    old_eff = h1c_eff.GetEfficiency(b)\n",
    "    new_eff = old_eff + corrections[b-1]\n",
    "    h1c_eff.GetPassedHistogram().SetBinContent(b, h1c_eff.GetTotalHistogram().GetBinContent(b) * new_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile1 = ROOT.TFile.Open(\"emtf_eff_vs_genpt_l1pt20.root\")\n",
    "frame = infile1.Get(\"emtf_eff_vs_genpt_l1pt20_denom_clone\")\n",
    "h1a_eff = infile1.Get(\"emtf_eff_vs_genpt_l1pt20_denom_clone\")\n",
    "h1b_eff = infile1.Get(\"emtf2023_eff_vs_genpt_l1pt20_denom_clone\")\n",
    "\n",
    "c = ROOT.TCanvas()\n",
    "frame.Draw()\n",
    "h1a_eff.Draw(\"same\")\n",
    "h1b_eff.Draw(\"same\")\n",
    "h1c_eff.Draw(\"same\")\n",
    "c.Draw()\n",
    "\n",
    "# Find corrections\n",
    "if True:\n",
    "  nbinsx = h1c_eff.GetTotalHistogram().GetNbinsX()\n",
    "  corrections = []\n",
    "  for b in xrange(1,nbinsx+1):\n",
    "    eff1 = h1b_eff.GetEfficiency(b)\n",
    "    eff2 = h1c_eff.GetEfficiency(b)\n",
    "    corr = eff1 - eff2\n",
    "    corrections.append(corr)\n",
    "  print corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Make trigger rates ####\n",
    "\n",
    "hname = \"highest_x_absEtaMin0_absEtaMax2.5_qmin12_pt\"\n",
    "rates_hist = Hist(100, 0., 100., name=hname, title=\"; p_{T} [GeV]; entries\", type='F')\n",
    "\n",
    "rates_nevents = 2000\n",
    "rates_njobs = 100\n",
    "rates_array = np.zeros((rates_njobs,rates_nevents), dtype=np.float32)\n",
    "\n",
    "for x, ndof, y_meas, y_discr, aux in zip(x_adv_test, x_adv_test, y_adv_test_meas[0], y_adv_test_meas[1], aux_adv_test):\n",
    "  \n",
    "  ndof = 4  #FIXME\n",
    "  \n",
    "  (jobid, ievt, highest_part_pt, highest_track_pt) = aux\n",
    "  jobid = int(jobid)\n",
    "  ievt = int(ievt)\n",
    "  \n",
    "  pt = mytrigger.get_trigger_pt(y_meas)\n",
    "  \n",
    "  trigger = mytrigger.pass_trigger(x, ndof, y_meas, y_discr)\n",
    "\n",
    "  if trigger:\n",
    "    rates_array[jobid,ievt] = max(rates_array[jobid,ievt], pt)\n",
    "\n",
    "rates_nevents_1 = 0\n",
    "\n",
    "for jobid in xrange(rates_array.shape[0]):\n",
    "  if rates_array[jobid].sum() > 0.:\n",
    "    for ievt in xrange(rates_array.shape[1]):\n",
    "      x = rates_array[jobid,ievt]\n",
    "      if x > 0.:\n",
    "        highest_pt = min(100.-1e-3, x)\n",
    "        rates_hist.fill(highest_pt)\n",
    "    rates_nevents_1 += rates_nevents\n",
    "\n",
    "print rates_nevents * rates_njobs, rates_nevents_1\n",
    "\n",
    "\n",
    "def make_ptcut(h):\n",
    "  use_overflow = True\n",
    "  binsum = 0\n",
    "  binerr2 = 0\n",
    "  for ib in xrange(h.GetNbinsX()+2-1, 0-1, -1):\n",
    "    if (not use_overflow) and (ib == 0 or ib == h.GetNbinsX()+1):\n",
    "      continue\n",
    "    binsum += h.GetBinContent(ib)\n",
    "    binerr2 += h.GetBinError(ib)**2\n",
    "    h.SetBinContent(ib, binsum)\n",
    "    h.SetBinError(ib, sqrt(binerr2))\n",
    "  return\n",
    "\n",
    "def make_rate(h, nevents):\n",
    "  orbitFreq = 11245.6\n",
    "  nCollBunches = 1866\n",
    "  nZeroBiasEvents = nevents\n",
    "  convFactorToHz = orbitFreq * nCollBunches / nZeroBiasEvents\n",
    "  h.Scale(convFactorToHz / 1000.)\n",
    "  return\n",
    "\n",
    "make_ptcut(rates_hist)\n",
    "make_rate(rates_hist, rates_nevents_1)\n",
    "\n",
    "rates_hist.SetLineColor(800)  # kOrange\n",
    "rates_hist.SetLineWidth(2)\n",
    "rates_hist.SetDirectory(0)\n",
    "histograms['rates_hist'] = rates_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = ROOT.TCanvas()\n",
    "rates_hist.Draw(\"hist\")\n",
    "c.SetLogy()\n",
    "c.Draw()\n",
    "print rates_hist.GetBinContent(rates_hist.FindBin(20.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Rates ####\n",
    "\n",
    "infile2 = ROOT.TFile.Open(\"emtf2023_rate_reduction.root\")\n",
    "#cc1 = infile2.Get(\"cc1\")\n",
    "denom = infile2.Get(\"denom\")\n",
    "numer = infile2.Get(\"numer\")\n",
    "ratio = infile2.Get(\"ratio\")\n",
    "\n",
    "rates_hist_ratio = rates_hist.Clone(\"ratio2\")\n",
    "rates_hist_ratio.Divide(rates_hist_ratio, denom, 1, 1, \"\")\n",
    "\n",
    "cc1 = ROOT.TCanvas(\"cc1\", \"cc1\", 600, 700)\n",
    "cc1.Divide(1,2)\n",
    "cc1_1 = cc1.GetPad(1)\n",
    "cc1_1.SetPad(0.01,0.25,0.99,0.99)\n",
    "cc1_1.SetBottomMargin(0.01)\n",
    "cc1_1.SetGrid()\n",
    "cc1_1.SetLogy()\n",
    "cc1_2 = cc1.GetPad(2)\n",
    "cc1_2.SetPad(0.01,0.01,0.99,0.25)\n",
    "cc1_2.SetTopMargin(0.01)\n",
    "cc1_2.SetBottomMargin(0.43)\n",
    "cc1_2.SetGrid()\n",
    "\n",
    "cc1_1.cd()\n",
    "denom.Draw(\"hist\")\n",
    "numer.Draw(\"hist same\")\n",
    "rates_hist.Draw(\"hist same\")\n",
    "cc1_2.cd()\n",
    "ratio.Draw(\"hist same\")\n",
    "rates_hist_ratio.Draw(\"hist same\")\n",
    "cc1.Draw()\n",
    "\n",
    "print denom.GetBinContent(denom.FindBin(20.))\n",
    "print numer.GetBinContent(numer.FindBin(20.))\n",
    "print rates_hist.GetBinContent(rates_hist.FindBin(20.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# from https://github.com/keras-team/keras/issues/4843\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.initializers import glorot_uniform, zero\n",
    "\n",
    "\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "x = K.placeholder(name=\"x\", shape=(None, input_dim))\n",
    "ytrue = K.placeholder(name=\"y\", shape=(None, output_dim))\n",
    "\n",
    "hidden_dim = 128\n",
    "W1 = K.variable(glorot_uniform()([input_dim, hidden_dim]))\n",
    "b1 = K.variable(zero()((hidden_dim,)))\n",
    "W2 = K.variable(glorot_uniform()([hidden_dim, output_dim]))\n",
    "b2 = K.variable(zero()((output_dim,)))\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "\n",
    "hidden = K.sigmoid(K.dot(x, W1)+b1)\n",
    "ypred = K.softmax(K.dot(hidden, W2)+b2)\n",
    "\n",
    "\n",
    "loss = K.mean(K.categorical_crossentropy(ytrue, ypred),axis=None)\n",
    "\n",
    "accuracy = categorical_accuracy(ytrue, ypred)\n",
    "\n",
    "opt = Adam()\n",
    "updates = opt.get_updates(params, [], loss, )\n",
    "train = K.function([x, ytrue],[loss, accuracy],updates=updates)\n",
    "\n",
    "test = K.function([x, ytrue], [loss, accuracy])\n",
    "\n",
    "((xtrain, ytrain),(xtest, ytest)) = mnist.load_data()\n",
    "(xtrain, xtest) = [x.reshape((-1, input_dim))/255.0 for x in (xtrain, xtest)]\n",
    "(ytrain, ytest) = [to_categorical(y, output_dim) for y in (ytrain, ytest)]\n",
    "for epoch in range(1000):\n",
    "\tloss, accuracy = train([xtrain, ytrain])\n",
    "\ttest_loss, test_accuracy = test([xtest, ytest])\n",
    "\tprint(\"Epoch: {}, Train Loss: {}, Train Accuracy: {}, Test Loss: {}, Test Accuracy: {}\".format(\n",
    "\t\tepoch, loss, accuracy, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
