{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DCGAN on MNIST using Keras\n",
    "Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "Dependencies: tensorflow 1.0 and keras 2.0\n",
    "Usage: python3 dcgan_mnist.py\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )\n",
    "\n",
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model\n",
    "        self.DM = None  # discriminator model\n",
    "\n",
    "    # (Wâˆ’F+2P)/S+1\n",
    "    def discriminator(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "        # In: 28 x 28 x 1, depth = 1\n",
    "        # Out: 14 x 14 x 1, depth=64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\\\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        # Out: 1-dim probability\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        dropout = 0.4\n",
    "        depth = 64+64+64+64\n",
    "        dim = 7\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "\n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AM\n",
    "\n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = input_data.read_data_sets(\"mnist\",\\\n",
    "            one_hot=True).train.images\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows,\\\n",
    "            self.img_cols, 1).astype(np.float32)\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0,\\\n",
    "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.694621, acc: 0.419922]  [A loss: 2.676497, acc: 0.000000]\n",
      "1: [D loss: 0.607144, acc: 0.960938]  [A loss: 4.553290, acc: 0.000000]\n",
      "2: [D loss: 0.455614, acc: 0.929688]  [A loss: 3.001086, acc: 0.000000]\n",
      "3: [D loss: 0.723727, acc: 0.503906]  [A loss: 14.802276, acc: 0.000000]\n",
      "4: [D loss: 0.558673, acc: 0.597656]  [A loss: 0.266460, acc: 0.980469]\n",
      "5: [D loss: 0.735753, acc: 0.515625]  [A loss: 9.640076, acc: 0.000000]\n",
      "6: [D loss: 0.119603, acc: 0.964844]  [A loss: 0.408136, acc: 0.843750]\n",
      "7: [D loss: 0.199394, acc: 0.955078]  [A loss: 3.075644, acc: 0.003906]\n",
      "8: [D loss: 0.203522, acc: 0.962891]  [A loss: 1.442636, acc: 0.242188]\n",
      "9: [D loss: 0.149468, acc: 0.996094]  [A loss: 0.122717, acc: 0.968750]\n",
      "10: [D loss: 0.106563, acc: 0.978516]  [A loss: 0.025869, acc: 1.000000]\n",
      "11: [D loss: 0.117887, acc: 0.980469]  [A loss: 0.011941, acc: 1.000000]\n",
      "12: [D loss: 0.102649, acc: 0.992188]  [A loss: 0.017403, acc: 1.000000]\n",
      "13: [D loss: 0.100991, acc: 0.986328]  [A loss: 0.008936, acc: 1.000000]\n",
      "14: [D loss: 0.099566, acc: 0.990234]  [A loss: 0.007199, acc: 1.000000]\n",
      "15: [D loss: 0.089037, acc: 0.998047]  [A loss: 0.010121, acc: 1.000000]\n",
      "16: [D loss: 0.087646, acc: 0.992188]  [A loss: 0.006454, acc: 1.000000]\n",
      "17: [D loss: 0.071815, acc: 1.000000]  [A loss: 0.019753, acc: 0.992188]\n",
      "18: [D loss: 0.064338, acc: 0.998047]  [A loss: 0.007914, acc: 1.000000]\n",
      "19: [D loss: 0.067715, acc: 0.994141]  [A loss: 0.002462, acc: 1.000000]\n",
      "20: [D loss: 0.077072, acc: 0.986328]  [A loss: 0.000799, acc: 1.000000]\n",
      "21: [D loss: 0.054732, acc: 0.998047]  [A loss: 0.003054, acc: 1.000000]\n",
      "22: [D loss: 0.047255, acc: 0.998047]  [A loss: 0.004578, acc: 1.000000]\n",
      "23: [D loss: 0.046100, acc: 0.994141]  [A loss: 0.001388, acc: 1.000000]\n",
      "24: [D loss: 0.042575, acc: 0.996094]  [A loss: 0.010548, acc: 0.992188]\n",
      "25: [D loss: 0.036011, acc: 1.000000]  [A loss: 0.011612, acc: 1.000000]\n",
      "26: [D loss: 0.031579, acc: 0.998047]  [A loss: 0.005755, acc: 1.000000]\n",
      "27: [D loss: 0.033595, acc: 0.996094]  [A loss: 0.002653, acc: 1.000000]\n",
      "28: [D loss: 0.035977, acc: 0.994141]  [A loss: 0.001005, acc: 1.000000]\n",
      "29: [D loss: 0.028361, acc: 0.998047]  [A loss: 0.007682, acc: 1.000000]\n",
      "30: [D loss: 0.033693, acc: 0.996094]  [A loss: 0.002672, acc: 1.000000]\n",
      "31: [D loss: 0.024633, acc: 1.000000]  [A loss: 0.073667, acc: 0.980469]\n",
      "32: [D loss: 0.094664, acc: 1.000000]  [A loss: 3.807991, acc: 0.167969]\n",
      "33: [D loss: 0.905032, acc: 0.537109]  [A loss: 16.118101, acc: 0.000000]\n",
      "34: [D loss: 3.450774, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "35: [D loss: 1.862618, acc: 0.505859]  [A loss: 0.000000, acc: 1.000000]\n",
      "36: [D loss: 0.660655, acc: 0.679688]  [A loss: 0.017910, acc: 0.996094]\n",
      "37: [D loss: 0.174708, acc: 0.939453]  [A loss: 0.356810, acc: 0.882812]\n",
      "38: [D loss: 0.438891, acc: 0.769531]  [A loss: 3.223970, acc: 0.246094]\n",
      "39: [D loss: 2.503597, acc: 0.507812]  [A loss: 11.668470, acc: 0.000000]\n",
      "40: [D loss: 1.762005, acc: 0.519531]  [A loss: 16.114981, acc: 0.000000]\n",
      "41: [D loss: 3.405734, acc: 0.500000]  [A loss: 0.000000, acc: 1.000000]\n",
      "42: [D loss: 3.865311, acc: 0.498047]  [A loss: 6.415699, acc: 0.015625]\n",
      "43: [D loss: 3.224373, acc: 0.501953]  [A loss: 15.736410, acc: 0.000000]\n",
      "44: [D loss: 1.750560, acc: 0.515625]  [A loss: 0.005057, acc: 0.996094]\n",
      "45: [D loss: 3.770331, acc: 0.494141]  [A loss: 16.067711, acc: 0.000000]\n",
      "46: [D loss: 3.383374, acc: 0.500000]  [A loss: 0.356664, acc: 0.855469]\n",
      "47: [D loss: 3.426165, acc: 0.492188]  [A loss: 16.114128, acc: 0.000000]\n",
      "48: [D loss: 3.955273, acc: 0.500000]  [A loss: 1.892133, acc: 0.433594]\n",
      "49: [D loss: 5.751650, acc: 0.496094]  [A loss: 13.277325, acc: 0.000000]\n",
      "50: [D loss: 2.446486, acc: 0.480469]  [A loss: 16.118101, acc: 0.000000]\n",
      "51: [D loss: 4.318570, acc: 0.500000]  [A loss: 5.440902, acc: 0.035156]\n",
      "52: [D loss: 6.189381, acc: 0.498047]  [A loss: 15.167246, acc: 0.000000]\n",
      "53: [D loss: 0.816742, acc: 0.654297]  [A loss: 1.160018, acc: 0.550781]\n",
      "54: [D loss: 4.399948, acc: 0.490234]  [A loss: 16.093002, acc: 0.000000]\n",
      "55: [D loss: 3.531406, acc: 0.500000]  [A loss: 3.633250, acc: 0.128906]\n",
      "56: [D loss: 5.974942, acc: 0.498047]  [A loss: 15.301239, acc: 0.000000]\n",
      "57: [D loss: 1.290775, acc: 0.576172]  [A loss: 0.525659, acc: 0.765625]\n",
      "58: [D loss: 3.736697, acc: 0.488281]  [A loss: 16.117718, acc: 0.000000]\n",
      "59: [D loss: 3.963184, acc: 0.500000]  [A loss: 5.956574, acc: 0.019531]\n",
      "60: [D loss: 5.787853, acc: 0.496094]  [A loss: 15.120242, acc: 0.000000]\n",
      "61: [D loss: 0.977941, acc: 0.636719]  [A loss: 0.796934, acc: 0.707031]\n",
      "62: [D loss: 4.222589, acc: 0.478516]  [A loss: 16.038248, acc: 0.000000]\n",
      "63: [D loss: 3.339869, acc: 0.500000]  [A loss: 4.453468, acc: 0.066406]\n",
      "64: [D loss: 5.991749, acc: 0.488281]  [A loss: 14.669464, acc: 0.000000]\n",
      "65: [D loss: 0.640307, acc: 0.738281]  [A loss: 2.291708, acc: 0.308594]\n",
      "66: [D loss: 4.851835, acc: 0.494141]  [A loss: 15.191488, acc: 0.000000]\n",
      "67: [D loss: 1.332235, acc: 0.566406]  [A loss: 1.103282, acc: 0.550781]\n",
      "68: [D loss: 3.112610, acc: 0.480469]  [A loss: 14.756541, acc: 0.000000]\n",
      "69: [D loss: 0.925192, acc: 0.650391]  [A loss: 0.856791, acc: 0.636719]\n",
      "70: [D loss: 2.858970, acc: 0.488281]  [A loss: 15.098313, acc: 0.000000]\n",
      "71: [D loss: 1.169298, acc: 0.611328]  [A loss: 0.757216, acc: 0.683594]\n",
      "72: [D loss: 2.948139, acc: 0.494141]  [A loss: 15.688763, acc: 0.000000]\n",
      "73: [D loss: 2.177351, acc: 0.515625]  [A loss: 1.993071, acc: 0.328125]\n",
      "74: [D loss: 4.415820, acc: 0.490234]  [A loss: 14.839831, acc: 0.000000]\n",
      "75: [D loss: 0.956810, acc: 0.691406]  [A loss: 0.610249, acc: 0.742188]\n",
      "76: [D loss: 3.298390, acc: 0.478516]  [A loss: 15.715084, acc: 0.000000]\n",
      "77: [D loss: 2.446598, acc: 0.525391]  [A loss: 1.146156, acc: 0.539062]\n",
      "78: [D loss: 4.307040, acc: 0.492188]  [A loss: 15.265364, acc: 0.000000]\n",
      "79: [D loss: 1.381400, acc: 0.613281]  [A loss: 0.619094, acc: 0.769531]\n",
      "80: [D loss: 3.621018, acc: 0.490234]  [A loss: 15.706127, acc: 0.000000]\n",
      "81: [D loss: 2.095927, acc: 0.542969]  [A loss: 1.196867, acc: 0.562500]\n",
      "82: [D loss: 4.841763, acc: 0.488281]  [A loss: 14.365099, acc: 0.000000]\n",
      "83: [D loss: 0.597207, acc: 0.779297]  [A loss: 1.885121, acc: 0.382812]\n",
      "84: [D loss: 4.829346, acc: 0.482422]  [A loss: 15.008010, acc: 0.000000]\n",
      "85: [D loss: 1.442985, acc: 0.642578]  [A loss: 0.447876, acc: 0.824219]\n",
      "86: [D loss: 3.087077, acc: 0.503906]  [A loss: 15.863203, acc: 0.000000]\n",
      "87: [D loss: 3.265720, acc: 0.515625]  [A loss: 2.276424, acc: 0.320312]\n",
      "88: [D loss: 6.076746, acc: 0.482422]  [A loss: 11.428997, acc: 0.000000]\n",
      "89: [D loss: 3.561541, acc: 0.451172]  [A loss: 15.506685, acc: 0.000000]\n",
      "90: [D loss: 2.514695, acc: 0.546875]  [A loss: 1.220799, acc: 0.593750]\n",
      "91: [D loss: 4.487274, acc: 0.482422]  [A loss: 14.848911, acc: 0.000000]\n",
      "92: [D loss: 1.244594, acc: 0.664062]  [A loss: 0.434138, acc: 0.832031]\n",
      "93: [D loss: 3.075988, acc: 0.496094]  [A loss: 15.426421, acc: 0.000000]\n",
      "94: [D loss: 2.238018, acc: 0.572266]  [A loss: 1.166996, acc: 0.550781]\n",
      "95: [D loss: 4.458718, acc: 0.492188]  [A loss: 14.741269, acc: 0.000000]\n",
      "96: [D loss: 1.478003, acc: 0.658203]  [A loss: 0.335421, acc: 0.878906]\n",
      "97: [D loss: 2.706504, acc: 0.523438]  [A loss: 14.865286, acc: 0.000000]\n",
      "98: [D loss: 1.575190, acc: 0.638672]  [A loss: 0.464987, acc: 0.843750]\n",
      "99: [D loss: 3.006724, acc: 0.509766]  [A loss: 14.477345, acc: 0.000000]\n",
      "100: [D loss: 1.341753, acc: 0.666016]  [A loss: 0.310606, acc: 0.878906]\n",
      "101: [D loss: 2.554605, acc: 0.537109]  [A loss: 14.060455, acc: 0.000000]\n",
      "102: [D loss: 0.917863, acc: 0.734375]  [A loss: 0.457706, acc: 0.839844]\n",
      "103: [D loss: 2.587075, acc: 0.558594]  [A loss: 13.290125, acc: 0.000000]\n",
      "104: [D loss: 0.636974, acc: 0.822266]  [A loss: 0.668770, acc: 0.750000]\n",
      "105: [D loss: 3.491581, acc: 0.511719]  [A loss: 13.764773, acc: 0.000000]\n",
      "106: [D loss: 1.384201, acc: 0.687500]  [A loss: 0.396413, acc: 0.890625]\n",
      "107: [D loss: 3.014546, acc: 0.511719]  [A loss: 13.091573, acc: 0.000000]\n",
      "108: [D loss: 1.046450, acc: 0.707031]  [A loss: 0.402229, acc: 0.863281]\n",
      "109: [D loss: 2.614939, acc: 0.546875]  [A loss: 11.876638, acc: 0.000000]\n",
      "110: [D loss: 0.586768, acc: 0.816406]  [A loss: 0.440913, acc: 0.859375]\n",
      "111: [D loss: 2.851297, acc: 0.533203]  [A loss: 12.540630, acc: 0.000000]\n",
      "112: [D loss: 0.976577, acc: 0.732422]  [A loss: 0.321853, acc: 0.867188]\n",
      "113: [D loss: 2.678727, acc: 0.535156]  [A loss: 10.952328, acc: 0.000000]\n",
      "114: [D loss: 0.660436, acc: 0.787109]  [A loss: 0.587825, acc: 0.773438]\n",
      "115: [D loss: 3.081810, acc: 0.515625]  [A loss: 10.632898, acc: 0.000000]\n",
      "116: [D loss: 0.521694, acc: 0.818359]  [A loss: 0.902722, acc: 0.632812]\n",
      "117: [D loss: 3.259502, acc: 0.509766]  [A loss: 10.147287, acc: 0.000000]\n",
      "118: [D loss: 0.616940, acc: 0.800781]  [A loss: 0.760686, acc: 0.730469]\n",
      "119: [D loss: 2.926850, acc: 0.509766]  [A loss: 8.164207, acc: 0.000000]\n",
      "120: [D loss: 0.658398, acc: 0.675781]  [A loss: 3.123368, acc: 0.046875]\n",
      "121: [D loss: 2.018431, acc: 0.501953]  [A loss: 4.762226, acc: 0.003906]\n",
      "122: [D loss: 1.363055, acc: 0.513672]  [A loss: 5.480391, acc: 0.000000]\n",
      "123: [D loss: 0.976721, acc: 0.537109]  [A loss: 3.034079, acc: 0.023438]\n",
      "124: [D loss: 1.221575, acc: 0.501953]  [A loss: 3.290168, acc: 0.019531]\n",
      "125: [D loss: 1.092840, acc: 0.492188]  [A loss: 1.954962, acc: 0.148438]\n",
      "126: [D loss: 1.066602, acc: 0.492188]  [A loss: 1.216990, acc: 0.308594]\n",
      "127: [D loss: 1.159700, acc: 0.494141]  [A loss: 1.781827, acc: 0.117188]\n",
      "128: [D loss: 1.112032, acc: 0.496094]  [A loss: 1.551877, acc: 0.171875]\n",
      "129: [D loss: 1.132587, acc: 0.496094]  [A loss: 1.895878, acc: 0.121094]\n",
      "130: [D loss: 1.030161, acc: 0.496094]  [A loss: 1.070749, acc: 0.363281]\n",
      "131: [D loss: 0.977429, acc: 0.496094]  [A loss: 1.161430, acc: 0.320312]\n",
      "132: [D loss: 0.985713, acc: 0.498047]  [A loss: 1.241068, acc: 0.253906]\n",
      "133: [D loss: 0.956985, acc: 0.496094]  [A loss: 1.548921, acc: 0.167969]\n",
      "134: [D loss: 0.907504, acc: 0.482422]  [A loss: 1.083218, acc: 0.308594]\n",
      "135: [D loss: 0.938052, acc: 0.496094]  [A loss: 1.411100, acc: 0.171875]\n",
      "136: [D loss: 0.915970, acc: 0.480469]  [A loss: 1.423908, acc: 0.160156]\n",
      "137: [D loss: 0.913174, acc: 0.501953]  [A loss: 1.637160, acc: 0.085938]\n",
      "138: [D loss: 0.911304, acc: 0.494141]  [A loss: 1.635450, acc: 0.101562]\n",
      "139: [D loss: 0.864437, acc: 0.492188]  [A loss: 1.357486, acc: 0.140625]\n",
      "140: [D loss: 1.008347, acc: 0.488281]  [A loss: 2.331820, acc: 0.023438]\n",
      "141: [D loss: 0.818591, acc: 0.480469]  [A loss: 1.363264, acc: 0.121094]\n",
      "142: [D loss: 0.943229, acc: 0.474609]  [A loss: 2.345999, acc: 0.015625]\n",
      "143: [D loss: 0.728081, acc: 0.509766]  [A loss: 1.125059, acc: 0.199219]\n",
      "144: [D loss: 0.791863, acc: 0.488281]  [A loss: 1.561123, acc: 0.074219]\n",
      "145: [D loss: 0.726923, acc: 0.509766]  [A loss: 1.626851, acc: 0.042969]\n",
      "146: [D loss: 0.837315, acc: 0.496094]  [A loss: 2.425126, acc: 0.003906]\n",
      "147: [D loss: 0.659190, acc: 0.517578]  [A loss: 1.188700, acc: 0.132812]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    mnist_dcgan = MNIST_DCGAN()\n",
    "    timer = ElapsedTimer()\n",
    "    mnist_dcgan.train(train_steps=10000, batch_size=256, save_interval=500)\n",
    "    timer.elapsed_time()\n",
    "    mnist_dcgan.plot_images(fake=True)\n",
    "    mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
